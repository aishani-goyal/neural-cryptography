{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmdK4n5Hi5V1",
        "outputId": "02ad6154-7c73-48ed-9e8f-0a71a630b729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "SIMPLIFIED ALICE ENCODER V2\n",
            "======================================================================\n",
            "\n",
            "Test messages:\n",
            "  1. 'Hello World!'\n",
            "  2. 'Testing encryption system.'\n",
            "  3. 'Short test'\n",
            "\n",
            "Encrypting...\n",
            "\n",
            "Encryption complete!\n",
            "  Input: 3 messages\n",
            "  Output shape: torch.Size([3, 64, 128])\n",
            "  Sample encrypted values: tensor([-0.4267, -0.5409, -0.4186, -0.3193,  0.5138,  0.2367, -0.3433, -0.9630,\n",
            "         0.1794, -0.2162])\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# ============ Key-XOR Layer (Explicit Key Operation) ============\n",
        "class KeyXORLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Applies key-conditioned XOR-like operation\n",
        "    This ensures encryption DEPENDS on keys\n",
        "    \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super(KeyXORLayer, self).__init__()\n",
        "        # Learnable projection for keys\n",
        "        self.key_proj = nn.Linear(1, dim)\n",
        "        self.mixing = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, key):\n",
        "        \"\"\"\n",
        "        x: (batch, seq_len, dim)\n",
        "        key: (batch, seq_len)\n",
        "        \"\"\"\n",
        "        # Project key to same dimension as x\n",
        "        key_expanded = key.unsqueeze(-1)  # (batch, seq_len, 1)\n",
        "        key_features = self.key_proj(key_expanded)  # (batch, seq_len, dim)\n",
        "\n",
        "        # XOR-like operation (element-wise multiplication + residual)\n",
        "        xor_out = x * torch.tanh(key_features)\n",
        "        mixed = self.mixing(xor_out)\n",
        "\n",
        "        return x + mixed  # Residual connection\n",
        "\n",
        "\n",
        "# ============ Residual Convolution Block ============\n",
        "class ResidualConvBlock(nn.Module):\n",
        "    \"\"\"Conv block with residual connection\"\"\"\n",
        "    def __init__(self, channels, kernel_size=3):\n",
        "        super(ResidualConvBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size//2)\n",
        "        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size//2)\n",
        "        self.norm1 = nn.LayerNorm(channels)\n",
        "        self.norm2 = nn.LayerNorm(channels)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"x: (batch, channels, seq_len)\"\"\"\n",
        "        residual = x\n",
        "\n",
        "        # First conv\n",
        "        out = self.conv1(x)\n",
        "        out = out.permute(0, 2, 1)  # (batch, seq_len, channels)\n",
        "        out = self.norm1(out)\n",
        "        out = out.permute(0, 2, 1)  # back to (batch, channels, seq_len)\n",
        "        out = F.relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Second conv\n",
        "        out = self.conv2(out)\n",
        "        out = out.permute(0, 2, 1)\n",
        "        out = self.norm2(out)\n",
        "        out = out.permute(0, 2, 1)\n",
        "\n",
        "        # Residual\n",
        "        return F.relu(out + residual)\n",
        "\n",
        "\n",
        "# ============ Simplified Alice Encoder ============\n",
        "class SimplifiedAliceEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Alice: Encodes and encrypts messages using keys\n",
        "    Key innovation: Explicit key-XOR operations at each layer\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=98, embed_dim=128, num_layers=3, max_len=128):\n",
        "        super(SimplifiedAliceEncoder, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Character embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        # Initial projection\n",
        "        self.input_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Encoding layers with key-XOR\n",
        "        self.conv_layers = nn.ModuleList([\n",
        "            ResidualConvBlock(embed_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.key_xor_layers = nn.ModuleList([\n",
        "            KeyXORLayer(embed_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection\n",
        "        self.output_proj = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "                module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "                if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                    module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, tokens, keys):\n",
        "        \"\"\"\n",
        "        tokens: (batch, seq_len) - integer indices\n",
        "        keys: list of 4 tensors, each (batch, seq_len)\n",
        "        Returns: (batch, seq_len, embed_dim) - encrypted embeddings\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = tokens.size()\n",
        "\n",
        "        # Embed tokens\n",
        "        x = self.embedding(tokens)  # (batch, seq_len, embed_dim)\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        # Process through layers with key conditioning\n",
        "        for i, (conv_layer, key_xor) in enumerate(zip(self.conv_layers, self.key_xor_layers)):\n",
        "            # Convolutional encoding\n",
        "            x_conv = x.permute(0, 2, 1)  # (batch, embed_dim, seq_len)\n",
        "            x_conv = conv_layer(x_conv)\n",
        "            x = x_conv.permute(0, 2, 1)  # (batch, seq_len, embed_dim)\n",
        "\n",
        "            # Apply key-XOR (this makes encryption depend on keys!)\n",
        "            x = key_xor(x, keys[i])\n",
        "\n",
        "        # Final projection\n",
        "        encrypted = self.output_proj(x)\n",
        "\n",
        "        return encrypted\n",
        "\n",
        "\n",
        "# ============ Alice Wrapper ============\n",
        "class AliceEncryptor:\n",
        "    \"\"\"Wrapper for Alice encoder\"\"\"\n",
        "    def __init__(self, vocab_size=98, embed_dim=128, num_layers=3, max_len=128,\n",
        "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.max_len = max_len\n",
        "        self.alice = SimplifiedAliceEncoder(vocab_size, embed_dim, num_layers, max_len).to(device)\n",
        "\n",
        "        from v2_part1_simplified import StringProcessor\n",
        "        self.processor = StringProcessor(max_length=max_len)\n",
        "\n",
        "    def encrypt(self, messages, keys):\n",
        "        \"\"\"\n",
        "        Encrypt messages (strings or tokens)\n",
        "        messages: list of strings OR tensor of tokens\n",
        "        keys: dict from KeyManager with 'key_tensors'\n",
        "        \"\"\"\n",
        "        # Convert strings to tokens if needed\n",
        "        if isinstance(messages, list) and isinstance(messages[0], str):\n",
        "            tokens = self.processor.batch_encode(messages).to(self.device)\n",
        "        else:\n",
        "            tokens = messages.to(self.device)\n",
        "\n",
        "        # Truncate keys to match sequence length\n",
        "        seq_len = tokens.size(1)\n",
        "        truncated_keys = [k[:, :seq_len] for k in keys['key_tensors']]\n",
        "\n",
        "        # Encrypt\n",
        "        self.alice.eval()\n",
        "        with torch.no_grad():\n",
        "            encrypted = self.alice(tokens, truncated_keys)\n",
        "\n",
        "        return encrypted\n",
        "\n",
        "\n",
        "# ============ Usage Example ============\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*70)\n",
        "    print(\"SIMPLIFIED ALICE ENCODER V2\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Initialize\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    from v2_part1_simplified import StringProcessor, KeyManager\n",
        "\n",
        "    processor = StringProcessor(max_length=64)\n",
        "    key_manager = KeyManager(device=device)\n",
        "    alice_enc = AliceEncryptor(\n",
        "        vocab_size=processor.vocab_size,\n",
        "        embed_dim=128,\n",
        "        num_layers=3,\n",
        "        max_len=64,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Test messages\n",
        "    messages = [\n",
        "        \"Hello World!\",\n",
        "        \"Testing encryption system.\",\n",
        "        \"Short test\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nTest messages:\")\n",
        "    for i, msg in enumerate(messages):\n",
        "        print(f\"  {i+1}. '{msg}'\")\n",
        "\n",
        "    # Generate keys\n",
        "    keys = key_manager.generate_keys_for_batch(\n",
        "        batch_size=len(messages),\n",
        "        seq_length=64\n",
        "    )\n",
        "\n",
        "    # Encrypt\n",
        "    print(\"\\nEncrypting...\")\n",
        "    encrypted = alice_enc.encrypt(messages, keys)\n",
        "\n",
        "    print(f\"\\nEncryption complete!\")\n",
        "    print(f\"  Input: {len(messages)} messages\")\n",
        "    print(f\"  Output shape: {encrypted.shape}\")\n",
        "    print(f\"  Sample encrypted values: {encrypted[0, 0, :10]}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rMtYw9bxjq9L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}