{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc_r5G_-jX8y",
        "outputId": "ee49c2d9-d86b-4fbc-dc69-a18730553e8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "FULL STAGED TRAINING\n",
            "Dataset: 8 messages\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "STAGE 1: TRAINING ALICE+BOB FOR RECONSTRUCTION\n",
            "======================================================================\n",
            "Epoch   0 | Loss: 4.5794 | Accuracy: 2.6%\n",
            "Epoch   0 | Loss: 4.5794 | Accuracy: 2.6% | Grad: 0.0581\n",
            "Epoch  10 | Loss: 3.3354 | Accuracy: 19.1%\n",
            "Epoch  10 | Loss: 3.3354 | Accuracy: 19.1% | Grad: 0.0001\n",
            "Epoch  20 | Loss: 3.0362 | Accuracy: 21.1%\n",
            "Epoch  20 | Loss: 3.0362 | Accuracy: 21.1% | Grad: 0.0009\n",
            "Epoch  30 | Loss: 2.8609 | Accuracy: 21.1%\n",
            "Epoch  30 | Loss: 2.8609 | Accuracy: 21.1% | Grad: 0.0005\n",
            "Epoch  40 | Loss: 2.7977 | Accuracy: 21.7%\n",
            "Epoch  40 | Loss: 2.7977 | Accuracy: 21.7% | Grad: 0.0016\n",
            "Epoch  50 | Loss: 2.5056 | Accuracy: 27.0%\n",
            "Epoch  50 | Loss: 2.5056 | Accuracy: 27.0% | Grad: 0.0525\n",
            "Epoch  60 | Loss: 2.0624 | Accuracy: 37.5%\n",
            "Epoch  60 | Loss: 2.0624 | Accuracy: 37.5% | Grad: 0.0306\n",
            "Epoch  70 | Loss: 1.6802 | Accuracy: 46.1%\n",
            "Epoch  70 | Loss: 1.6802 | Accuracy: 46.1% | Grad: 0.0965\n",
            "Epoch  80 | Loss: 1.1277 | Accuracy: 66.4%\n",
            "Epoch  80 | Loss: 1.1277 | Accuracy: 66.4% | Grad: 0.0278\n",
            "Epoch  90 | Loss: 0.6509 | Accuracy: 82.9%\n",
            "Epoch  90 | Loss: 0.6509 | Accuracy: 82.9% | Grad: 0.0405\n",
            "\n",
            "Stage 1 Complete! Final accuracy: 92.2%\n",
            "SUCCESS: Bob can decrypt accurately!\n",
            "\n",
            "======================================================================\n",
            "STAGE 2: TRAINING EVE (ADVERSARY)\n",
            "======================================================================\n",
            "Epoch   0 | Loss: 4.6061 | Eve Accuracy: 0.0%\n",
            "Epoch  10 | Loss: 4.0484 | Eve Accuracy: 28.4%\n",
            "Epoch  20 | Loss: 3.5943 | Eve Accuracy: 31.6%\n",
            "\n",
            "Stage 2 Complete! Eve trained.\n",
            "\n",
            "======================================================================\n",
            "STAGE 3: ADVERSARIAL FINE-TUNING\n",
            "======================================================================\n",
            "Epoch   0 | Bob: 0.281 (97.2%) | Eve: 3.370 (32.4%) | Ratio: 12.01x\n",
            "Epoch  10 | Bob: 0.765 (79.2%) | Eve: 2.979 (43.1%) | Ratio: 3.89x\n",
            "Epoch  20 | Bob: 0.218 (95.9%) | Eve: 2.651 (45.2%) | Ratio: 12.15x\n",
            "Epoch  30 | Bob: 0.127 (100.0%) | Eve: 2.232 (55.6%) | Ratio: 17.61x\n",
            "Epoch  40 | Bob: 0.043 (100.0%) | Eve: 2.096 (59.7%) | Ratio: 48.38x\n",
            "\n",
            "Stage 3 Complete!\n",
            "\n",
            "======================================================================\n",
            "FINAL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Example 1:\n",
            "  Original: 'Hello World!'\n",
            "  Bob:      'Hello borld!' (91.7%)\n",
            "  Eve:      'eeteo to ' (38.1%)\n",
            "\n",
            "Example 2:\n",
            "  Original: 'This is a test.'\n",
            "  Bob:      'This is a test.' (100.0%)\n",
            "  Eve:      'Thes es   test.' (80.0%)\n",
            "\n",
            "Example 3:\n",
            "  Original: 'Secret message here.'\n",
            "  Bob:      'Secret message here.' (100.0%)\n",
            "  Eve:      'Tesret oesr  e here.' (70.0%)\n",
            "\n",
            "Example 4:\n",
            "  Original: 'Encryption works!'\n",
            "  Bob:      'Encryption works!' (100.0%)\n",
            "  Eve:      'ees s tere s rrs.' (23.5%)\n",
            "\n",
            "Example 5:\n",
            "  Original: 'Neural crypto system.'\n",
            "  Bob:      'Neural crypto system.' (100.0%)\n",
            "  Eve:      'Tec  t s s to s stes.' (47.6%)\n",
            "\n",
            "======================================================================\n",
            "Bob Similarity: 98.3% ✓\n",
            "Eve Similarity: 51.8% ✗\n",
            "Security Ratio: 1.90x\n",
            "======================================================================\n",
            "\n",
            "✓ Training complete!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# ============ Staged Training System ============\n",
        "class StagedCryptoTrainer:\n",
        "    \"\"\"\n",
        "    3-stage training approach:\n",
        "    Stage 1: Train Alice+Bob for perfect reconstruction (no Eve)\n",
        "    Stage 2: Train Eve to attack (freeze Alice+Bob)\n",
        "    Stage 3: Adversarial fine-tuning (all together)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=98, embed_dim=128, alice_layers=3,\n",
        "                 bob_layers=3, eve_layers=4, max_len=128,\n",
        "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.max_len = max_len\n",
        "\n",
        "        # Import components\n",
        "        from v2_part1_simplified import StringProcessor, KeyManager\n",
        "        from v2_part2_alice import AliceEncryptor\n",
        "        from v2_part3_bob_eve import BobDecryptor, EveAttacker\n",
        "\n",
        "        self.processor = StringProcessor(max_length=max_len)\n",
        "        self.key_manager = KeyManager(device=device)\n",
        "\n",
        "        # Initialize networks\n",
        "        self.alice_wrapper = AliceEncryptor(vocab_size, embed_dim, alice_layers, max_len, device)\n",
        "        self.bob_wrapper = BobDecryptor(vocab_size, embed_dim, bob_layers, max_len, device)\n",
        "        self.eve_wrapper = EveAttacker(vocab_size, embed_dim, eve_layers, max_len, device)\n",
        "\n",
        "        self.alice = self.alice_wrapper.alice\n",
        "        self.bob = self.bob_wrapper.bob\n",
        "        self.eve = self.eve_wrapper.eve\n",
        "\n",
        "        # Optimizers\n",
        "        self.opt_alice = optim.Adam(self.alice.parameters(), lr=0.003)\n",
        "        self.opt_bob = optim.Adam(self.bob.parameters(), lr=0.003)\n",
        "        self.opt_eve = optim.Adam(self.eve.parameters(), lr=0.0005)\n",
        "\n",
        "        # Schedulers\n",
        "        self.sched_alice = optim.lr_scheduler.ReduceLROnPlateau(self.opt_alice, patience=5, factor=0.5)\n",
        "        self.sched_bob = optim.lr_scheduler.ReduceLROnPlateau(self.opt_bob, patience=5, factor=0.5)\n",
        "        self.sched_eve = optim.lr_scheduler.ReduceLROnPlateau(self.opt_eve, patience=5, factor=0.8)\n",
        "\n",
        "        # Loss\n",
        "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "        self.current_stage = 1\n",
        "\n",
        "    def train_stage1_reconstruction(self, messages, num_epochs=50):\n",
        "        \"\"\"\n",
        "        Stage 1: Train Alice+Bob for perfect reconstruction\n",
        "        Goal: Bob accuracy > 90%\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"STAGE 1: TRAINING ALICE+BOB FOR RECONSTRUCTION\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        history = {'loss': [], 'accuracy': []}\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            # Sample batch\n",
        "            batch = np.random.choice(messages, size=min(8, len(messages)), replace=False).tolist()\n",
        "            tokens = self.processor.batch_encode(batch).to(self.device)\n",
        "\n",
        "            # Generate keys\n",
        "            keys = self.key_manager.generate_keys_for_batch(len(batch), tokens.size(1))\n",
        "\n",
        "            # Forward pass\n",
        "            self.opt_alice.zero_grad()\n",
        "            self.opt_bob.zero_grad()\n",
        "\n",
        "            self.alice.train()\n",
        "            self.bob.train()\n",
        "\n",
        "            encrypted = self.alice(tokens, keys['key_tensors'])\n",
        "            logits = self.bob(encrypted, keys['key_tensors'])\n",
        "\n",
        "            # Reconstruction loss\n",
        "            loss = self.ce_loss(logits.reshape(-1, logits.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.alice.parameters(), 1.0)\n",
        "            torch.nn.utils.clip_grad_norm_(self.bob.parameters(), 1.0)\n",
        "\n",
        "            self.opt_alice.step()\n",
        "            self.opt_bob.step()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            with torch.no_grad():\n",
        "                pred_tokens = torch.argmax(logits, dim=-1)\n",
        "                mask = tokens != 0\n",
        "                correct = ((pred_tokens == tokens) & mask).sum().item()\n",
        "                total = mask.sum().item()\n",
        "                accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "            history['loss'].append(loss.item())\n",
        "            history['accuracy'].append(accuracy)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | Accuracy: {accuracy*100:.1f}%\")\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                total_grad = sum(p.grad.abs().mean().item()\n",
        "                              for p in self.alice.parameters() if p.grad is not None)\n",
        "                print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | \"\n",
        "                  f\"Accuracy: {accuracy*100:.1f}% | Grad: {total_grad:.4f}\")\n",
        "\n",
        "            # Update learning rate\n",
        "            self.sched_alice.step(loss)\n",
        "            self.sched_bob.step(loss)\n",
        "\n",
        "        final_acc = np.mean(history['accuracy'][-10:])\n",
        "        print(f\"\\nStage 1 Complete! Final accuracy: {final_acc*100:.1f}%\")\n",
        "\n",
        "        if final_acc > 0.90:\n",
        "            print(\"SUCCESS: Bob can decrypt accurately!\")\n",
        "            self.current_stage = 2\n",
        "        else:\n",
        "            print(\"WARNING: Bob accuracy < 90%. Consider training longer.\")\n",
        "\n",
        "        return history\n",
        "\n",
        "    def train_stage2_adversary(self, messages, num_epochs=30):\n",
        "        \"\"\"\n",
        "        Stage 2: Train Eve to attack (freeze Alice+Bob)\n",
        "        Goal: Eve learns patterns but can't match Bob\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"STAGE 2: TRAINING EVE (ADVERSARY)\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        # Freeze Alice and Bob\n",
        "        for param in self.alice.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.bob.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        history = {'loss': [], 'accuracy': []}\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            batch = np.random.choice(messages, size=min(4, len(messages)), replace=False).tolist()\n",
        "            tokens = self.processor.batch_encode(batch).to(self.device)\n",
        "\n",
        "            keys = self.key_manager.generate_keys_for_batch(len(batch), tokens.size(1))\n",
        "\n",
        "            # Get encrypted data from Alice\n",
        "            with torch.no_grad():\n",
        "                encrypted = self.alice(tokens, keys['key_tensors'])\n",
        "\n",
        "            # Train Eve\n",
        "            self.opt_eve.zero_grad()\n",
        "            self.eve.train()\n",
        "\n",
        "            eve_logits = self.eve(encrypted)\n",
        "            loss = self.ce_loss(eve_logits.reshape(-1, eve_logits.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.eve.parameters(), 1.0)\n",
        "            self.opt_eve.step()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            with torch.no_grad():\n",
        "                pred_tokens = torch.argmax(eve_logits, dim=-1)\n",
        "                mask = tokens != 0\n",
        "                correct = ((pred_tokens == tokens) & mask).sum().item()\n",
        "                total = mask.sum().item()\n",
        "                accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "            history['loss'].append(loss.item())\n",
        "            history['accuracy'].append(accuracy)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | Eve Accuracy: {accuracy*100:.1f}%\")\n",
        "\n",
        "            self.sched_eve.step(loss)\n",
        "\n",
        "        # Unfreeze Alice and Bob\n",
        "        for param in self.alice.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in self.bob.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        print(f\"\\nStage 2 Complete! Eve trained.\")\n",
        "        self.current_stage = 3\n",
        "\n",
        "        return history\n",
        "\n",
        "    def train_stage3_adversarial(self, messages, num_epochs=50):\n",
        "        \"\"\"\n",
        "        Stage 3: Adversarial fine-tuning\n",
        "        Goal: Alice learns to fool Eve while Bob still decrypts\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"STAGE 3: ADVERSARIAL FINE-TUNING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        history = {\n",
        "            'bob_loss': [], 'eve_loss': [],\n",
        "            'bob_acc': [], 'eve_acc': [], 'ratio': []\n",
        "        }\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            batch = np.random.choice(messages, size=min(4, len(messages)), replace=False).tolist()\n",
        "            tokens = self.processor.batch_encode(batch).to(self.device)\n",
        "            keys = self.key_manager.generate_keys_for_batch(len(batch), tokens.size(1))\n",
        "\n",
        "            # Train Alice+Bob\n",
        "            self.opt_alice.zero_grad()\n",
        "            self.opt_bob.zero_grad()\n",
        "\n",
        "            self.alice.train()\n",
        "            self.bob.train()\n",
        "\n",
        "            encrypted = self.alice(tokens, keys['key_tensors'])\n",
        "            bob_logits = self.bob(encrypted, keys['key_tensors'])\n",
        "            bob_loss = self.ce_loss(bob_logits.reshape(-1, bob_logits.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            bob_loss.backward()\n",
        "            self.opt_alice.step()\n",
        "            self.opt_bob.step()\n",
        "\n",
        "            # Train Eve\n",
        "            self.opt_eve.zero_grad()\n",
        "            self.eve.train()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                encrypted_for_eve = self.alice(tokens, keys['key_tensors'])\n",
        "\n",
        "            eve_logits = self.eve(encrypted_for_eve)\n",
        "            eve_loss = self.ce_loss(eve_logits.reshape(-1, eve_logits.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            eve_loss.backward()\n",
        "            self.opt_eve.step()\n",
        "\n",
        "            # Adversarial update for Alice (make Eve fail)\n",
        "            self.opt_alice.zero_grad()\n",
        "\n",
        "            encrypted_adv = self.alice(tokens, keys['key_tensors'])\n",
        "            eve_logits_adv = self.eve(encrypted_adv)\n",
        "            adv_loss = -self.ce_loss(eve_logits_adv.reshape(-1, eve_logits_adv.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            (adv_loss * 0.5).backward()  # Weight adversarial loss lower\n",
        "            self.opt_alice.step()\n",
        "\n",
        "            # Calculate metrics\n",
        "            with torch.no_grad():\n",
        "                bob_pred = torch.argmax(bob_logits, dim=-1)\n",
        "                eve_pred = torch.argmax(eve_logits, dim=-1)\n",
        "                mask = tokens != 0\n",
        "\n",
        "                bob_acc = ((bob_pred == tokens) & mask).sum().item() / mask.sum().item()\n",
        "                eve_acc = ((eve_pred == tokens) & mask).sum().item() / mask.sum().item()\n",
        "                ratio = eve_loss.item() / (bob_loss.item() + 1e-8)\n",
        "\n",
        "            history['bob_loss'].append(bob_loss.item())\n",
        "            history['eve_loss'].append(eve_loss.item())\n",
        "            history['bob_acc'].append(bob_acc)\n",
        "            history['eve_acc'].append(eve_acc)\n",
        "            history['ratio'].append(ratio)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch:3d} | Bob: {bob_loss.item():.3f} ({bob_acc*100:.1f}%) | \"\n",
        "                      f\"Eve: {eve_loss.item():.3f} ({eve_acc*100:.1f}%) | Ratio: {ratio:.2f}x\")\n",
        "\n",
        "        print(f\"\\nStage 3 Complete!\")\n",
        "        return history\n",
        "\n",
        "    def full_training(self, messages, stage1_epochs=50, stage2_epochs=30, stage3_epochs=50):\n",
        "        \"\"\"Run all 3 stages\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"FULL STAGED TRAINING\")\n",
        "        print(f\"Dataset: {len(messages)} messages\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        h1 = self.train_stage1_reconstruction(messages, stage1_epochs)\n",
        "        h2 = self.train_stage2_adversary(messages, stage2_epochs)\n",
        "        h3 = self.train_stage3_adversarial(messages, stage3_epochs)\n",
        "\n",
        "        return {'stage1': h1, 'stage2': h2, 'stage3': h3}\n",
        "\n",
        "\n",
        "# ============ Evaluation ============\n",
        "class CryptoEvaluator:\n",
        "    @staticmethod\n",
        "    def evaluate(trainer, test_messages):\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"FINAL EVALUATION\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        results = {'bob_sim': [], 'eve_sim': [], 'examples': []}\n",
        "\n",
        "        for i, original in enumerate(test_messages[:5]):\n",
        "            tokens = trainer.processor.encode(original).unsqueeze(0).to(trainer.device)\n",
        "            keys = trainer.key_manager.generate_keys_for_batch(1, tokens.size(1))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                encrypted = trainer.alice(tokens, keys['key_tensors'])\n",
        "                bob_logits = trainer.bob(encrypted, keys['key_tensors'])\n",
        "                eve_logits = trainer.eve(encrypted)\n",
        "\n",
        "                bob_tokens = torch.argmax(bob_logits, dim=-1)\n",
        "                eve_tokens = torch.argmax(eve_logits, dim=-1)\n",
        "\n",
        "                bob_msg = trainer.processor.decode(bob_tokens[0])\n",
        "                eve_msg = trainer.processor.decode(eve_tokens[0])\n",
        "\n",
        "            bob_sim = SequenceMatcher(None, original, bob_msg).ratio()\n",
        "            eve_sim = SequenceMatcher(None, original, eve_msg).ratio()\n",
        "\n",
        "            results['bob_sim'].append(bob_sim)\n",
        "            results['eve_sim'].append(eve_sim)\n",
        "            results['examples'].append((original, bob_msg, eve_msg))\n",
        "\n",
        "            print(f\"\\nExample {i+1}:\")\n",
        "            print(f\"  Original: '{original}'\")\n",
        "            print(f\"  Bob:      '{bob_msg}' ({bob_sim*100:.1f}%)\")\n",
        "            print(f\"  Eve:      '{eve_msg}' ({eve_sim*100:.1f}%)\")\n",
        "\n",
        "        avg_bob = np.mean(results['bob_sim'])\n",
        "        avg_eve = np.mean(results['eve_sim'])\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"Bob Similarity: {avg_bob*100:.1f}% {'✓' if avg_bob > 0.9 else '✗'}\")\n",
        "        print(f\"Eve Similarity: {avg_eve*100:.1f}% {'✓' if avg_eve < 0.3 else '✗'}\")\n",
        "        print(f\"Security Ratio: {avg_bob/max(avg_eve,0.01):.2f}x\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "# ============ Main ============\n",
        "if __name__ == \"__main__\":\n",
        "    # Dataset\n",
        "    messages = np.array([\n",
        "        \"Hello World!\",\n",
        "        \"This is a test.\",\n",
        "        \"Secret message here.\",\n",
        "        \"Encryption works!\",\n",
        "        \"Neural crypto system.\",\n",
        "        \"Testing ABC 123.\",\n",
        "        \"Quick brown fox.\",\n",
        "        \"The lazy dog jumps.\"\n",
        "    ])\n",
        "\n",
        "    # Initialize\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    trainer = StagedCryptoTrainer(\n",
        "        vocab_size=98,\n",
        "        embed_dim=128,\n",
        "        alice_layers=3,\n",
        "        bob_layers=3,\n",
        "        eve_layers=4,\n",
        "        max_len=64,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    history = trainer.full_training(\n",
        "        messages,\n",
        "        stage1_epochs=100,  # Focus on reconstruction\n",
        "        stage2_epochs=30,\n",
        "        stage3_epochs=50\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    results = CryptoEvaluator.evaluate(trainer, messages)\n",
        "\n",
        "    print(\"\\n✓ Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test file: test_reconstruction.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MinimalAutoencoder(nn.Module):\n",
        "    def __init__(self, vocab_size=98, embed_dim=32):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.encoder = nn.Linear(embed_dim, embed_dim)\n",
        "        self.decoder = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        x = self.embedding(tokens)\n",
        "        x = torch.relu(self.encoder(x))\n",
        "        logits = self.decoder(x)\n",
        "        return logits\n",
        "\n",
        "# Test\n",
        "model = MinimalAutoencoder().cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Very simple test\n",
        "test_tokens = torch.tensor([[1, 40, 69, 76, 76, 79, 2, 0]]).cuda()  # \"Hello\"\n",
        "\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(test_tokens)\n",
        "    loss = criterion(logits.reshape(-1, 98), test_tokens.reshape(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        pred = torch.argmax(logits, dim=-1)\n",
        "        acc = (pred == test_tokens).float().mean().item()\n",
        "        print(f\"Epoch {epoch}: Loss {loss.item():.3f}, Acc {acc*100:.1f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_DICthTkDxh",
        "outputId": "ccb2f812-2e3f-4b41-8104-1e1b01e9a3c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss 4.738, Acc 0.0%\n",
            "Epoch 20: Loss 0.008, Acc 100.0%\n",
            "Epoch 40: Loss 0.000, Acc 100.0%\n",
            "Epoch 60: Loss 0.000, Acc 100.0%\n",
            "Epoch 80: Loss 0.000, Acc 100.0%\n",
            "Epoch 100: Loss 0.000, Acc 100.0%\n",
            "Epoch 120: Loss 0.000, Acc 100.0%\n",
            "Epoch 140: Loss 0.000, Acc 100.0%\n",
            "Epoch 160: Loss 0.000, Acc 100.0%\n",
            "Epoch 180: Loss 0.000, Acc 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tafSHVCunyZB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}