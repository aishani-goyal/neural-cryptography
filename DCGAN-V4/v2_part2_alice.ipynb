{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmdK4n5Hi5V1",
        "outputId": "194d7852-af0f-4575-8895-f459a156ea24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "SIMPLIFIED ALICE ENCODER V2\n",
            "======================================================================\n",
            "\n",
            "Test messages:\n",
            "  1. 'Hello World!'\n",
            "  2. 'Testing encryption system.'\n",
            "  3. 'Short test'\n",
            "\n",
            "Encrypting...\n",
            "\n",
            "Encryption complete!\n",
            "  Input: 3 messages\n",
            "  Output shape: torch.Size([3, 64, 128])\n",
            "  Sample encrypted values: tensor([-0.1212,  0.0563, -0.0203, -0.0992, -0.7033, -0.0784, -0.2577,  0.1227,\n",
            "        -0.0676, -0.3457])\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# ============ Key-XOR Layer (Explicit Key Operation) ============\n",
        "class KeyXORLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Applies key-conditioned XOR-like operation\n",
        "    This ensures encryption DEPENDS on keys\n",
        "    \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super(KeyXORLayer, self).__init__()\n",
        "        # Learnable projection for keys\n",
        "        self.key_proj = nn.Linear(1, dim)\n",
        "        self.mixing = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, key):\n",
        "        \"\"\"\n",
        "        x: (batch, seq_len, dim)\n",
        "        key: (batch, seq_len)\n",
        "        \"\"\"\n",
        "        # Project key to same dimension as x\n",
        "        key_expanded = key.unsqueeze(-1)  # (batch, seq_len, 1)\n",
        "        key_features = self.key_proj(key_expanded)  # (batch, seq_len, dim)\n",
        "\n",
        "        # XOR-like operation (element-wise multiplication + residual)\n",
        "        xor_out = x * torch.tanh(key_features)\n",
        "        mixed = self.mixing(xor_out)\n",
        "\n",
        "        return x + mixed  # Residual connection\n",
        "\n",
        "\n",
        "# ============ Residual Convolution Block ============\n",
        "class ResidualConvBlock(nn.Module):\n",
        "    \"\"\"Conv block with residual connection\"\"\"\n",
        "    def __init__(self, channels, kernel_size=3):\n",
        "        super(ResidualConvBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size//2)\n",
        "        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding=kernel_size//2)\n",
        "        self.norm1 = nn.LayerNorm(channels)\n",
        "        self.norm2 = nn.LayerNorm(channels)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"x: (batch, channels, seq_len)\"\"\"\n",
        "        residual = x\n",
        "\n",
        "        # First conv\n",
        "        out = self.conv1(x)\n",
        "        out = out.permute(0, 2, 1)  # (batch, seq_len, channels)\n",
        "        out = self.norm1(out)\n",
        "        out = out.permute(0, 2, 1)  # back to (batch, channels, seq_len)\n",
        "        out = F.relu(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # Second conv\n",
        "        out = self.conv2(out)\n",
        "        out = out.permute(0, 2, 1)\n",
        "        out = self.norm2(out)\n",
        "        out = out.permute(0, 2, 1)\n",
        "\n",
        "        # Residual\n",
        "        return F.relu(out + residual)\n",
        "\n",
        "\n",
        "# ============ Simplified Alice Encoder ============\n",
        "class SimplifiedAliceEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size=98, embed_dim=64, num_layers=1, max_len=128):  # Changed!\n",
        "        super(SimplifiedAliceEncoder, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Character embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        # SINGLE encoding layer\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(embed_dim * 2, embed_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Single key mixing\n",
        "        self.key_mixer = nn.Linear(embed_dim + 1, embed_dim)\n",
        "\n",
        "    def forward(self, tokens, keys):\n",
        "        batch_size, seq_len = tokens.size()\n",
        "\n",
        "        # Embed\n",
        "        x = self.embedding(tokens)  # (batch, seq_len, embed_dim)\n",
        "\n",
        "        # Simple key integration\n",
        "        key_expanded = keys[0].unsqueeze(-1)  # Use only first key\n",
        "        x_with_key = torch.cat([x, key_expanded], dim=-1)\n",
        "        x_mixed = self.key_mixer(x_with_key)\n",
        "\n",
        "        # Encode\n",
        "        x = self.encoder(x + x_mixed)  # Residual\n",
        "\n",
        "        return x\n",
        "\n",
        "# ============ Alice Wrapper ============\n",
        "class AliceEncryptor:\n",
        "    \"\"\"Wrapper for Alice encoder\"\"\"\n",
        "    def __init__(self, vocab_size=98, embed_dim=128, num_layers=3, max_len=128,\n",
        "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.max_len = max_len\n",
        "        self.alice = SimplifiedAliceEncoder(vocab_size, embed_dim, num_layers, max_len).to(device)\n",
        "\n",
        "        from v2_part1_simplified import StringProcessor\n",
        "        self.processor = StringProcessor(max_length=max_len)\n",
        "\n",
        "    def encrypt(self, messages, keys):\n",
        "        \"\"\"\n",
        "        Encrypt messages (strings or tokens)\n",
        "        messages: list of strings OR tensor of tokens\n",
        "        keys: dict from KeyManager with 'key_tensors'\n",
        "        \"\"\"\n",
        "        # Convert strings to tokens if needed\n",
        "        if isinstance(messages, list) and isinstance(messages[0], str):\n",
        "            tokens = self.processor.batch_encode(messages).to(self.device)\n",
        "        else:\n",
        "            tokens = messages.to(self.device)\n",
        "\n",
        "        # Truncate keys to match sequence length\n",
        "        seq_len = tokens.size(1)\n",
        "        truncated_keys = [k[:, :seq_len] for k in keys['key_tensors']]\n",
        "\n",
        "        # Encrypt\n",
        "        self.alice.eval()\n",
        "        with torch.no_grad():\n",
        "            encrypted = self.alice(tokens, truncated_keys)\n",
        "\n",
        "        return encrypted\n",
        "\n",
        "\n",
        "# ============ Usage Example ============\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*70)\n",
        "    print(\"SIMPLIFIED ALICE ENCODER V2\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Initialize\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    from v2_part1_simplified import StringProcessor, KeyManager\n",
        "\n",
        "    processor = StringProcessor(max_length=64)\n",
        "    key_manager = KeyManager(device=device)\n",
        "    alice_enc = AliceEncryptor(\n",
        "        vocab_size=processor.vocab_size,\n",
        "        embed_dim=128,\n",
        "        num_layers=3,\n",
        "        max_len=64,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Test messages\n",
        "    messages = [\n",
        "        \"Hello World!\",\n",
        "        \"Testing encryption system.\",\n",
        "        \"Short test\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nTest messages:\")\n",
        "    for i, msg in enumerate(messages):\n",
        "        print(f\"  {i+1}. '{msg}'\")\n",
        "\n",
        "    # Generate keys\n",
        "    keys = key_manager.generate_keys_for_batch(\n",
        "        batch_size=len(messages),\n",
        "        seq_length=64\n",
        "    )\n",
        "\n",
        "    # Encrypt\n",
        "    print(\"\\nEncrypting...\")\n",
        "    encrypted = alice_enc.encrypt(messages, keys)\n",
        "\n",
        "    print(f\"\\nEncryption complete!\")\n",
        "    print(f\"  Input: {len(messages)} messages\")\n",
        "    print(f\"  Output shape: {encrypted.shape}\")\n",
        "    print(f\"  Sample encrypted values: {encrypted[0, 0, :10]}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rMtYw9bxjq9L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}