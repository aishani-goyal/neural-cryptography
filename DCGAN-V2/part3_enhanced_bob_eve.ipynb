{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5_WlPf9-Leb",
        "outputId": "7de1e02b-9a6d-4fdd-e4fe-eee7ae4e96b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ENHANCED BOB DECODER & EVE ATTACKER\n",
            "======================================================================\n",
            "\n",
            "Bob attempting decryption with correct keys...\n",
            "Bob's output: 'k\u000bk\u000b\u000bk\u000bkkkk\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000b\u000bk\u000b\u000b\u000b\u0019\u000b\u000b\u000bk\u000b\u0019\u000b\u000b\u000bk\u0019\u0019\u000b\u000b\u0019k\u0019\u000b\u0019\u000bk...'\n",
            "\n",
            "Eve attempting to break encryption without keys...\n",
            "Eve's output: 'sssssssssssssssssssssssssss$ssssssssssssssssssssss...'\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# Import from Part 2\n",
        "from part2_enhanced_alice import KeyConditionedAttention, PositionalEncoding\n",
        "\n",
        "# ============ Decoder Block with Key Conditioning ============\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"Transformer decoder block - must have correct keys to decrypt\"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        # Self-attention on encrypted input\n",
        "        self.self_attn = KeyConditionedAttention(d_model, num_heads)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Key mixing layer (decryption depends on keys)\n",
        "        self.key_mixer = nn.Sequential(\n",
        "            nn.Linear(d_model * 2, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, key_features):\n",
        "        \"\"\"\n",
        "        x: (batch, seq_len, d_model) - encrypted sequence\n",
        "        key_features: (batch, d_model) - decryption keys\n",
        "        \"\"\"\n",
        "        # Self-attention with key conditioning\n",
        "        attn_out = self.self_attn(x, x, x, key_features)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "\n",
        "        # Mix with key features (decryption step)\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        key_expanded = key_features.unsqueeze(1).expand(-1, seq_len, -1)\n",
        "        x_with_key = torch.cat([x, key_expanded], dim=-1)\n",
        "        key_mixed = self.key_mixer(x_with_key)\n",
        "        x = x + key_mixed\n",
        "\n",
        "        # Feed-forward\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + ffn_out)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============ Enhanced Bob Decoder ============\n",
        "class EnhancedBobDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Bob: Decrypts encrypted messages using correct keys\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=256, d_model=256, num_heads=8,\n",
        "                 num_layers=6, d_ff=1024, max_len=512, dropout=0.1):\n",
        "        super(EnhancedBobDecoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Input projection\n",
        "        self.input_projection = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Key encoder (must match Alice's key encoding)\n",
        "        self.key_encoder = nn.Sequential(\n",
        "            nn.Linear(512, d_model * 2),\n",
        "            nn.LayerNorm(d_model * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model * 2, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Stack of decoder blocks\n",
        "        self.decoder_blocks = nn.ModuleList([\n",
        "            DecoderBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output head to predict characters\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, vocab_size)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, encrypted_sequence, key_sequences):\n",
        "        \"\"\"\n",
        "        encrypted_sequence: (batch, seq_len, d_model) - from Alice\n",
        "        key_sequences: same keys used for encryption\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = encrypted_sequence.size()\n",
        "        device = encrypted_sequence.device\n",
        "\n",
        "        # Combine key sequences\n",
        "        if isinstance(key_sequences[0], np.ndarray):\n",
        "            key_combined = np.concatenate([ks[:512] for ks in key_sequences[:1]], axis=0)\n",
        "            key_tensor = torch.FloatTensor(key_combined).to(device)\n",
        "        else:\n",
        "            key_combined = torch.cat([ks[:512] for ks in key_sequences[:1]], dim=0).to(device)\n",
        "            key_tensor = key_combined\n",
        "\n",
        "        # Expand for batch\n",
        "        key_tensor = key_tensor.unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Encode keys (same as Alice)\n",
        "        key_features = self.key_encoder(key_tensor)\n",
        "\n",
        "        # Project encrypted input\n",
        "        x = self.input_projection(encrypted_sequence)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through decoder blocks with key conditioning\n",
        "        for decoder_block in self.decoder_blocks:\n",
        "            x = decoder_block(x, key_features)\n",
        "\n",
        "        # Predict characters\n",
        "        logits = self.output_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ============ Enhanced Eve Attacker ============\n",
        "class EnhancedEveAttacker(nn.Module):\n",
        "    \"\"\"\n",
        "    Eve: Tries to decrypt WITHOUT keys (adversary)\n",
        "    Uses a different architecture to avoid copying Alice/Bob\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=256, d_model=256, num_heads=8,\n",
        "                 num_layers=8, d_ff=1024, max_len=512, dropout=0.3):\n",
        "        super(EnhancedEveAttacker, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Input projection\n",
        "        self.input_projection = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 2),\n",
        "            nn.LayerNorm(d_model * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model * 2, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Powerful transformer layers (Eve uses more layers)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=d_ff,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        # Output head\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.LayerNorm(d_model // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_model // 2, vocab_size)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, encrypted_sequence):\n",
        "        \"\"\"\n",
        "        encrypted_sequence: (batch, seq_len, d_model)\n",
        "        No keys provided - Eve must break it without keys\n",
        "        \"\"\"\n",
        "        # Project input\n",
        "        x = self.input_projection(encrypted_sequence)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Transform\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Predict characters\n",
        "        logits = self.output_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ============ Bob Decryption Module ============\n",
        "class BobStringDecryption:\n",
        "    \"\"\"Wrapper for Bob decoder with string support\"\"\"\n",
        "    def __init__(self, vocab_size=256, d_model=256, num_heads=8,\n",
        "                 num_layers=6, max_len=512,\n",
        "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.bob = EnhancedBobDecoder(\n",
        "            vocab_size=vocab_size,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "            max_len=max_len\n",
        "        ).to(device)\n",
        "\n",
        "        from part1_enhanced_dcgan import StringPreprocessor\n",
        "        self.preprocessor = StringPreprocessor(max_length=max_len)\n",
        "\n",
        "    def decrypt_to_string(self, encrypted_sequence, key_sequences):\n",
        "        \"\"\"\n",
        "        Decrypt encrypted sequence back to string\n",
        "        encrypted_sequence: (batch, seq_len, d_model)\n",
        "        key_sequences: same keys used for encryption\n",
        "        \"\"\"\n",
        "        self.bob.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = self.bob(encrypted_sequence, key_sequences)\n",
        "            predicted_tokens = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        # Convert tokens to strings\n",
        "        decrypted_strings = self.preprocessor.batch_tensor_to_strings(predicted_tokens)\n",
        "\n",
        "        return decrypted_strings, predicted_tokens, logits\n",
        "\n",
        "\n",
        "# ============ Eve Attack Module ============\n",
        "class EveStringAttack:\n",
        "    \"\"\"Wrapper for Eve attacker with string support\"\"\"\n",
        "    def __init__(self, vocab_size=256, d_model=256, num_heads=8,\n",
        "                 num_layers=8, max_len=512,\n",
        "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.eve = EnhancedEveAttacker(\n",
        "            vocab_size=vocab_size,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "            max_len=max_len\n",
        "        ).to(device)\n",
        "\n",
        "        from part1_enhanced_dcgan import StringPreprocessor\n",
        "        self.preprocessor = StringPreprocessor(max_length=max_len)\n",
        "\n",
        "    def attack_to_string(self, encrypted_sequence):\n",
        "        \"\"\"\n",
        "        Try to decrypt without keys\n",
        "        encrypted_sequence: (batch, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        self.eve.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = self.eve(encrypted_sequence)\n",
        "            predicted_tokens = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        # Convert tokens to strings\n",
        "        attacked_strings = self.preprocessor.batch_tensor_to_strings(predicted_tokens)\n",
        "\n",
        "        return attacked_strings, predicted_tokens, logits\n",
        "\n",
        "\n",
        "# ============ Usage Example ============\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*70)\n",
        "    print(\"ENHANCED BOB DECODER & EVE ATTACKER\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Initialize\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    bob_dec = BobStringDecryption(device=device)\n",
        "    eve_att = EveStringAttack(device=device)\n",
        "\n",
        "    # Simulate encrypted sequence\n",
        "    batch_size = 2\n",
        "    seq_len = 256\n",
        "    d_model = 256\n",
        "    encrypted = torch.randn(batch_size, seq_len, d_model).to(device)\n",
        "\n",
        "    # Simulate keys\n",
        "    key_sequences = [np.random.rand(512) for _ in range(8)]\n",
        "\n",
        "    print(\"\\nBob attempting decryption with correct keys...\")\n",
        "    bob_strings, bob_tokens, _ = bob_dec.decrypt_to_string(encrypted, key_sequences)\n",
        "    print(f\"Bob's output: '{bob_strings[0][:50]}...'\")\n",
        "\n",
        "    print(\"\\nEve attempting to break encryption without keys...\")\n",
        "    eve_strings, eve_tokens, _ = eve_att.attack_to_string(encrypted)\n",
        "    print(f\"Eve's output: '{eve_strings[0][:50]}...'\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "75MPy73Y-UJG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}