{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0LkZx6C9vEO",
        "outputId": "7b33d39a-6a2c-496a-bbff-ea3d671177a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ENHANCED ALICE ENCODER - STRING ENCRYPTION\n",
            "======================================================================\n",
            "\n",
            "Original message: 'Hello World! This is a secret message.'\n",
            "Encrypting...\n",
            "\n",
            "Encryption complete!\n",
            "  Input length: 38 characters\n",
            "  Encrypted shape: torch.Size([1, 512, 256])\n",
            "  Encrypted tensor (first 10 values):\n",
            "  tensor([ 0.2025, -0.2799,  0.6672,  0.5963,  0.4357,  0.6974,  0.2273, -0.4616,\n",
            "         0.4486,  0.6217])\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "# ============ Positional Encoding ============\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Add positional information to embeddings\"\"\"\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "\n",
        "# ============ Key-Conditioned Multi-Head Attention ============\n",
        "class KeyConditionedAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention that is conditioned on encryption keys\n",
        "    This ensures keys are DEEPLY integrated into the encryption\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, num_heads=8):\n",
        "        super(KeyConditionedAttention, self).__init__()\n",
        "\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        # Query, Key, Value projections\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Key conditioning layer\n",
        "        self.key_projection = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output projection\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, query, key, value, key_condition):\n",
        "        \"\"\"\n",
        "        query, key, value: (batch, seq_len, d_model)\n",
        "        key_condition: (batch, d_model) - encryption key features\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Project encryption key\n",
        "        key_features = self.key_projection(key_condition).unsqueeze(1)  # (batch, 1, d_model)\n",
        "\n",
        "        # Add key conditioning to queries (this makes attention key-dependent)\n",
        "        query = query + key_features\n",
        "\n",
        "        # Multi-head projections\n",
        "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Apply attention to values\n",
        "        context = torch.matmul(attn, V)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # Output projection\n",
        "        output = self.W_o(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# ============ Encoder Block with Key Conditioning ============\n",
        "class EncoderBlock(nn.Module):\n",
        "    \"\"\"Transformer encoder block with deep key integration\"\"\"\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "\n",
        "        # Key-conditioned self-attention\n",
        "        self.self_attn = KeyConditionedAttention(d_model, num_heads)\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Key mixing layer\n",
        "        self.key_mixer = nn.Sequential(\n",
        "            nn.Linear(d_model * 2, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, key_features):\n",
        "        \"\"\"\n",
        "        x: (batch, seq_len, d_model)\n",
        "        key_features: (batch, d_model)\n",
        "        \"\"\"\n",
        "        # Self-attention with key conditioning\n",
        "        attn_out = self.self_attn(x, x, x, key_features)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "\n",
        "        # Mix with key features\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        key_expanded = key_features.unsqueeze(1).expand(-1, seq_len, -1)\n",
        "        x_with_key = torch.cat([x, key_expanded], dim=-1)\n",
        "        key_mixed = self.key_mixer(x_with_key)\n",
        "        x = x + key_mixed\n",
        "\n",
        "        # Feed-forward\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + ffn_out)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# ============ Enhanced Alice Encoder ============\n",
        "class EnhancedAliceEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Alice: Encrypts string messages using transformer architecture\n",
        "    with deep key conditioning\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=256, d_model=256, num_heads=8,\n",
        "                 num_layers=6, d_ff=1024, max_len=512, dropout=0.1):\n",
        "        super(EnhancedAliceEncoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Embedding layer for characters\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "\n",
        "        # Key encoder - processes encryption keys\n",
        "        self.key_encoder = nn.Sequential(\n",
        "            nn.Linear(512, d_model * 2),\n",
        "            nn.LayerNorm(d_model * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model * 2, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        # Stack of encoder blocks\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            EncoderBlock(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection to encrypted sequence\n",
        "        self.output_projection = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.LayerNorm(d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, message_tokens, key_sequences):\n",
        "        \"\"\"\n",
        "        message_tokens: (batch, seq_len) - integer tokens\n",
        "        key_sequences: list of 8 numpy arrays or tensors\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = message_tokens.size()\n",
        "        device = message_tokens.device\n",
        "\n",
        "        # Combine all key sequences into a single feature vector\n",
        "        if isinstance(key_sequences[0], np.ndarray):\n",
        "            key_combined = np.concatenate([ks[:512] for ks in key_sequences[:1]], axis=0)  # Use first key\n",
        "            key_tensor = torch.FloatTensor(key_combined).to(device)\n",
        "        else:\n",
        "            key_combined = torch.cat([ks[:512] for ks in key_sequences[:1]], dim=0).to(device)\n",
        "            key_tensor = key_combined\n",
        "\n",
        "        # Expand for batch\n",
        "        key_tensor = key_tensor.unsqueeze(0).expand(batch_size, -1)\n",
        "\n",
        "        # Encode keys\n",
        "        key_features = self.key_encoder(key_tensor)  # (batch, d_model)\n",
        "\n",
        "        # Embed message tokens\n",
        "        x = self.embedding(message_tokens) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass through encoder blocks with key conditioning\n",
        "        for encoder_block in self.encoder_blocks:\n",
        "            x = encoder_block(x, key_features)\n",
        "\n",
        "        # Project to encrypted representation\n",
        "        encrypted = self.output_projection(x)\n",
        "\n",
        "        return encrypted\n",
        "\n",
        "\n",
        "# ============ Alice Encryption Module ============\n",
        "class AliceStringEncryption:\n",
        "    \"\"\"Wrapper for Alice encoder with string support\"\"\"\n",
        "    def __init__(self, vocab_size=256, d_model=256, num_heads=8,\n",
        "                 num_layers=6, max_len=512,\n",
        "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.alice = EnhancedAliceEncoder(\n",
        "            vocab_size=vocab_size,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "            max_len=max_len\n",
        "        ).to(device)\n",
        "\n",
        "        from part1_enhanced_dcgan import StringPreprocessor\n",
        "        self.preprocessor = StringPreprocessor(max_length=max_len)\n",
        "\n",
        "    def encrypt_string(self, message_string, key_sequences):\n",
        "        \"\"\"\n",
        "        Encrypt a string message\n",
        "        message_string: str or list of str\n",
        "        key_sequences: from DCGAN key generator\n",
        "        \"\"\"\n",
        "        # Convert string to tensor\n",
        "        if isinstance(message_string, str):\n",
        "            message_tensor = self.preprocessor.string_to_tensor(message_string).unsqueeze(0)\n",
        "        else:\n",
        "            message_tensor = self.preprocessor.batch_strings_to_tensor(message_string)\n",
        "\n",
        "        message_tensor = message_tensor.to(self.device)\n",
        "\n",
        "        # Encrypt\n",
        "        self.alice.eval()\n",
        "        with torch.no_grad():\n",
        "            encrypted = self.alice(message_tensor, key_sequences)\n",
        "\n",
        "        return encrypted\n",
        "\n",
        "    def encrypt_tensor(self, message_tokens, key_sequences):\n",
        "        \"\"\"Encrypt already tokenized message\"\"\"\n",
        "        self.alice.eval()\n",
        "        with torch.no_grad():\n",
        "            encrypted = self.alice(message_tokens, key_sequences)\n",
        "        return encrypted\n",
        "\n",
        "\n",
        "# ============ Usage Example ============\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*70)\n",
        "    print(\"ENHANCED ALICE ENCODER - STRING ENCRYPTION\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Initialize\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    alice_enc = AliceStringEncryption(device=device)\n",
        "\n",
        "    # Test message\n",
        "    test_message = \"Hello World! This is a secret message.\"\n",
        "    print(f\"\\nOriginal message: '{test_message}'\")\n",
        "\n",
        "    # Generate keys (simulated)\n",
        "    key_sequences = [np.random.rand(512) for _ in range(8)]\n",
        "\n",
        "    # Encrypt\n",
        "    print(\"Encrypting...\")\n",
        "    encrypted = alice_enc.encrypt_string(test_message, key_sequences)\n",
        "\n",
        "    print(f\"\\nEncryption complete!\")\n",
        "    print(f\"  Input length: {len(test_message)} characters\")\n",
        "    print(f\"  Encrypted shape: {encrypted.shape}\")\n",
        "    print(f\"  Encrypted tensor (first 10 values):\")\n",
        "    print(f\"  {encrypted[0, 0, :10]}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DOugS7_g9zN7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}