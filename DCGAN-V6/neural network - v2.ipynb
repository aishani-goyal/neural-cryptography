{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VW_dEX3vkGj",
    "outputId": "2820a31c-4354-452d-eed3-f88b0e8c2837"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Dataset: 54 messages\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PHASE 1: RECONSTRUCTION TRAINING (NO ENCRYPTION)\n",
      "======================================================================\n",
      "Epoch   0 | Loss: 0.6586 | Acc: 89.3%\n",
      "Epoch  20 | Loss: 0.0003 | Acc: 100.0%\n",
      "Epoch  40 | Loss: 0.0001 | Acc: 100.0%\n",
      "Epoch  60 | Loss: 0.0000 | Acc: 100.0%\n",
      "Epoch  80 | Loss: 0.0000 | Acc: 100.0%\n",
      "Epoch 100 | Loss: 0.0000 | Acc: 100.0%\n",
      "Epoch 120 | Loss: 0.0000 | Acc: 100.0%\n",
      "Epoch 140 | Loss: 0.0000 | Acc: 100.0%\n",
      "Epoch 160 | Loss: 0.0000 | Acc: 100.0%\n",
      "Epoch 180 | Loss: 0.0000 | Acc: 100.0%\n",
      "\n",
      "✓ Phase 1 Complete! Accuracy: 100.0%\n",
      "\n",
      "======================================================================\n",
      "PHASE 2: GENTLE ENCRYPTION TRAINING\n",
      "======================================================================\n",
      "Epoch   0 | Bob: 2.712 (66.0%) | Eve: 4.276 (0.8%) | Ratio: 1.58x\n",
      "Epoch  10 | Bob: 0.038 (99.0%) | Eve: 2.012 (71.7%) | Ratio: 52.84x\n",
      "Epoch  20 | Bob: 0.009 (99.8%) | Eve: 1.429 (79.5%) | Ratio: 156.48x\n",
      "Epoch  30 | Bob: 0.010 (99.8%) | Eve: 0.986 (89.5%) | Ratio: 98.11x\n",
      "Epoch  40 | Bob: 0.001 (100.0%) | Eve: 1.082 (81.2%) | Ratio: 904.85x\n",
      "Epoch  50 | Bob: 0.009 (99.8%) | Eve: 1.424 (70.1%) | Ratio: 164.23x\n",
      "Epoch  60 | Bob: 0.014 (99.8%) | Eve: 3.372 (10.9%) | Ratio: 236.03x\n",
      "Epoch  70 | Bob: 0.003 (100.0%) | Eve: 1.638 (71.1%) | Ratio: 507.08x\n",
      "Epoch  80 | Bob: 0.004 (100.0%) | Eve: 1.453 (73.8%) | Ratio: 351.35x\n",
      "Epoch  90 | Bob: 0.005 (100.0%) | Eve: 1.796 (63.3%) | Ratio: 382.93x\n",
      "\n",
      "✓ Phase 2 Complete!\n",
      "\n",
      "======================================================================\n",
      "FINAL EVALUATION\n",
      "======================================================================\n",
      "\n",
      "Original:  'Hello World!'\n",
      "Bob:       'Hello World!' (100.0%)\n",
      "Eve:       '' (0.0%)\n",
      "Wrong key: Avg 21.5% similarity\n",
      "\n",
      "Original:  'This is a test.'\n",
      "Bob:       'This is a test.' (100.0%)\n",
      "Eve:       'ii' (23.5%)\n",
      "Wrong key: Avg 6.9% similarity\n",
      "\n",
      "Original:  'Secret message here.'\n",
      "Bob:       'Secret message here.' (100.0%)\n",
      "Eve:       '' (0.0%)\n",
      "Wrong key: Avg 9.4% similarity\n",
      "\n",
      "Original:  'Encryption works!'\n",
      "Bob:       'Encryption works!' (100.0%)\n",
      "Eve:       'i' (11.1%)\n",
      "Wrong key: Avg 13.5% similarity\n",
      "\n",
      "Original:  'Neural crypto system.'\n",
      "Bob:       'Neural crypto system.' (100.0%)\n",
      "Eve:       '' (0.0%)\n",
      "Wrong key: Avg 14.1% similarity\n",
      "\n",
      "Original:  'Testing ABC 123.'\n",
      "Bob:       'Testing ABC 123.' (100.0%)\n",
      "Eve:       'ii' (11.1%)\n",
      "Wrong key: Avg 7.6% similarity\n",
      "\n",
      "Original:  'Quick brown fox.'\n",
      "Bob:       'Quick brown fox.' (100.0%)\n",
      "Eve:       'i' (11.8%)\n",
      "Wrong key: Avg 3.3% similarity\n",
      "\n",
      "Original:  'The lazy dog jumps.'\n",
      "Bob:       'The lazy dog jumps.' (100.0%)\n",
      "Eve:       '' (0.0%)\n",
      "Wrong key: Avg 12.3% similarity\n",
      "\n",
      "Original:  'Machine learning is powerful.'\n",
      "Bob:       'Machine learning is powerful.' (100.0%)\n",
      "Eve:       'iii' (18.8%)\n",
      "Wrong key: Avg 14.6% similarity\n",
      "\n",
      "Original:  'Deep neural networks.'\n",
      "Bob:       'Deep neural networks.' (100.0%)\n",
      "Eve:       '' (0.0%)\n",
      "Wrong key: Avg 6.6% similarity\n",
      "\n",
      "======================================================================\n",
      "FINAL METRICS\n",
      "======================================================================\n",
      "Bob Similarity:    100.0% ✓\n",
      "Eve Similarity:    7.6% ✓\n",
      "Key Sensitivity:   89.0% ✓\n",
      "Security Ratio:    13.11x ✓\n",
      "\n",
      "✓ Bob: EXCELLENT decryption!\n",
      "✓ Eve: CANNOT break encryption!\n",
      "✓ Keys: Good sensitivity!\n",
      "\n",
      "🎉 SUCCESS! System works well!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "MINIMAL FIX VERSION\n",
    "Strategy: Keep the original working system, just tweak key sensitivity\n",
    "Don't break what's already working!\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import hashlib\n",
    "\n",
    "# ============ String Processor (UNCHANGED) ============\n",
    "class StringProcessor:\n",
    "    def __init__(self, max_len=64):\n",
    "        self.max_len = max_len\n",
    "        chars = ' abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,!?-'\n",
    "        self.vocab = {c: i for i, c in enumerate(chars)}\n",
    "        self.vocab['<PAD>'] = len(self.vocab)\n",
    "        self.vocab['<END>'] = len(self.vocab)\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def encode(self, text):\n",
    "        ids = [self.vocab.get(c, 0) for c in text[:self.max_len-1]]\n",
    "        ids.append(self.vocab['<END>'])\n",
    "        while len(ids) < self.max_len:\n",
    "            ids.append(self.vocab['<PAD>'])\n",
    "        return torch.LongTensor(ids)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        chars = []\n",
    "        for i in ids:\n",
    "            c = self.inv_vocab.get(int(i), '')\n",
    "            if c == '<END>':\n",
    "                break\n",
    "            if c != '<PAD>':\n",
    "                chars.append(c)\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def batch_encode(self, texts):\n",
    "        return torch.stack([self.encode(t) for t in texts])\n",
    "\n",
    "\n",
    "# ============ Autoencoder (UNCHANGED) ============\n",
    "class CryptoAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab_size-2)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens, return_embeddings=False):\n",
    "        emb = self.embedding(tokens)\n",
    "        enc = self.encoder(emb)\n",
    "\n",
    "        if return_embeddings:\n",
    "            return enc\n",
    "\n",
    "        logits = self.decoder(enc)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ============ Key Generator (UNCHANGED) ============\n",
    "class KeyGenerator:\n",
    "    @staticmethod\n",
    "    def generate_from_message(message, key_size=128):\n",
    "        msg_hash = hashlib.sha256(message.encode()).hexdigest()\n",
    "        seed = int(msg_hash[:8], 16)\n",
    "        np.random.seed(seed)\n",
    "        key = torch.FloatTensor(np.random.randn(key_size))\n",
    "        return key\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_random(key_size=128):\n",
    "        return torch.randn(key_size)\n",
    "\n",
    "\n",
    "# ============ TWEAKED: Slightly Stronger Key Layer ============\n",
    "class KeyDependentEncryption(nn.Module):\n",
    "    \"\"\"MINIMAL CHANGE: Just add one more nonlinearity\"\"\"\n",
    "    def __init__(self, embed_dim=128):\n",
    "        super().__init__()\n",
    "        # Original transform + one extra layer\n",
    "        self.key_transform = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim * 2, embed_dim),\n",
    "            nn.Tanh()  # Extra nonlinearity\n",
    "        )\n",
    "\n",
    "        self.encrypt_mix = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, embed_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encrypt(self, embeddings, key):\n",
    "        if len(key.shape) == 1:\n",
    "            key = key.unsqueeze(0).expand(embeddings.size(0), -1)\n",
    "\n",
    "        key_features = self.key_transform(key)\n",
    "        key_expanded = key_features.unsqueeze(1).expand(-1, embeddings.size(1), -1)\n",
    "\n",
    "        combined = torch.cat([embeddings, key_expanded], dim=-1)\n",
    "        encrypted = self.encrypt_mix(combined)\n",
    "\n",
    "        # SLIGHT TWEAK: Multiply by 2 for stronger key effect\n",
    "        encrypted = encrypted * (1 + key_expanded * 2)\n",
    "\n",
    "        return encrypted\n",
    "\n",
    "    def decrypt(self, encrypted, key):\n",
    "        if len(key.shape) == 1:\n",
    "            key = key.unsqueeze(0).expand(encrypted.size(0), -1)\n",
    "\n",
    "        key_features = self.key_transform(key)\n",
    "        key_expanded = key_features.unsqueeze(1).expand(-1, encrypted.size(1), -1)\n",
    "\n",
    "        # Reverse with same factor\n",
    "        decrypted = encrypted / (1 + key_expanded * 2 + 1e-8)\n",
    "\n",
    "        return decrypted\n",
    "\n",
    "\n",
    "# ============ Eve (UNCHANGED) ============\n",
    "class EveAttacker(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.attack_network = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 2),\n",
    "            nn.LayerNorm(embed_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(embed_dim * 2, embed_dim * 2),\n",
    "            nn.LayerNorm(embed_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(embed_dim * 2, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, encrypted_embeddings):\n",
    "        return self.attack_network(encrypted_embeddings)\n",
    "\n",
    "\n",
    "# ============ CRITICAL FIX: Gentler Phase 2 Training ============\n",
    "class NeuralCryptoSystem:\n",
    "    def __init__(self, vocab_size, embed_dim=128, device='cuda'):\n",
    "        self.device = device\n",
    "        self.processor = StringProcessor()\n",
    "\n",
    "        self.autoencoder = CryptoAutoencoder(vocab_size, embed_dim).to(device)\n",
    "        self.crypto_layer = KeyDependentEncryption(embed_dim).to(device)\n",
    "        self.eve = EveAttacker(vocab_size, embed_dim).to(device)\n",
    "\n",
    "        self.opt_main = optim.Adam(\n",
    "            list(self.autoencoder.parameters()) + list(self.crypto_layer.parameters()),\n",
    "            lr=0.001\n",
    "        )\n",
    "        self.opt_eve = optim.Adam(self.eve.parameters(), lr=0.0005)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train_phase1_reconstruction(self, messages, epochs=200):\n",
    "        \"\"\"UNCHANGED - This works perfectly\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PHASE 1: RECONSTRUCTION TRAINING (NO ENCRYPTION)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "\n",
    "            for msg in messages:\n",
    "                tokens = self.processor.encode(msg).unsqueeze(0).to(self.device)\n",
    "\n",
    "                self.opt_main.zero_grad()\n",
    "                logits = self.autoencoder(tokens)\n",
    "\n",
    "                loss = self.criterion(logits.view(-1, logits.size(-1)), tokens.view(-1))\n",
    "                loss.backward()\n",
    "                self.opt_main.step()\n",
    "\n",
    "                pred = torch.argmax(logits, dim=-1)\n",
    "                acc = (pred == tokens).float().mean().item()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_acc += acc\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                avg_acc = total_acc / len(messages)\n",
    "                print(f\"Epoch {epoch:3d} | Loss: {total_loss/len(messages):.4f} | Acc: {avg_acc*100:.1f}%\")\n",
    "\n",
    "        final_acc = total_acc / len(messages)\n",
    "        print(f\"\\n✓ Phase 1 Complete! Accuracy: {final_acc*100:.1f}%\")\n",
    "        return final_acc > 0.95\n",
    "\n",
    "    def train_phase2_with_encryption(self, messages, epochs=100):\n",
    "        \"\"\"\n",
    "        CRITICAL FIX: Gentle training that preserves Phase 1 learning\n",
    "        - Keep original training intensity\n",
    "        - Just add small adversarial component\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PHASE 2: GENTLE ENCRYPTION TRAINING\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batch_msgs = np.random.choice(messages, min(8, len(messages)), replace=True)\n",
    "            tokens = self.processor.batch_encode(batch_msgs).to(self.device)\n",
    "\n",
    "            keys = torch.stack([KeyGenerator.generate_random(128) for _ in batch_msgs]).to(self.device)\n",
    "\n",
    "            # === Train Alice+Bob (ORIGINAL intensity) ===\n",
    "            self.opt_main.zero_grad()\n",
    "\n",
    "            embeddings = self.autoencoder(tokens, return_embeddings=True)\n",
    "            encrypted = self.crypto_layer.encrypt(embeddings, keys)\n",
    "            decrypted = self.crypto_layer.decrypt(encrypted, keys)\n",
    "            logits = self.autoencoder.decoder(decrypted)\n",
    "\n",
    "            loss_reconstruction = self.criterion(logits.view(-1, logits.size(-1)), tokens.view(-1))\n",
    "            loss_reconstruction.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.autoencoder.parameters(), 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(self.crypto_layer.parameters(), 1.0)\n",
    "            self.opt_main.step()\n",
    "\n",
    "            # === Train Eve (ORIGINAL intensity) ===\n",
    "            self.opt_eve.zero_grad()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                embeddings = self.autoencoder(tokens, return_embeddings=True)\n",
    "                encrypted = self.crypto_layer.encrypt(embeddings, keys)\n",
    "\n",
    "            eve_logits = self.eve(encrypted)\n",
    "            loss_eve = self.criterion(eve_logits.view(-1, eve_logits.size(-1)), tokens.view(-1))\n",
    "            loss_eve.backward()\n",
    "            self.opt_eve.step()\n",
    "\n",
    "            # === MINIMAL Adversarial (only after epoch 30) ===\n",
    "            if epoch > 30:  # Wait until Bob is stable!\n",
    "                self.opt_main.zero_grad()\n",
    "\n",
    "                embeddings = self.autoencoder(tokens, return_embeddings=True)\n",
    "                encrypted = self.crypto_layer.encrypt(embeddings, keys)\n",
    "                eve_attack = self.eve(encrypted)\n",
    "\n",
    "                loss_adversarial = -self.criterion(eve_attack.view(-1, eve_attack.size(-1)), tokens.view(-1))\n",
    "                (loss_adversarial * 0.1).backward()  # VERY SMALL weight!\n",
    "                self.opt_main.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                bob_pred = torch.argmax(logits, dim=-1)\n",
    "                eve_pred = torch.argmax(eve_logits, dim=-1)\n",
    "\n",
    "                bob_acc = (bob_pred == tokens).float().mean().item()\n",
    "                eve_acc = (eve_pred == tokens).float().mean().item()\n",
    "                ratio = loss_eve.item() / (loss_reconstruction.item() + 1e-8)\n",
    "\n",
    "                print(f\"Epoch {epoch:3d} | Bob: {loss_reconstruction.item():.3f} ({bob_acc*100:.1f}%) | \"\n",
    "                      f\"Eve: {loss_eve.item():.3f} ({eve_acc*100:.1f}%) | Ratio: {ratio:.2f}x\")\n",
    "\n",
    "        print(\"\\n✓ Phase 2 Complete!\")\n",
    "\n",
    "    def encrypt_message(self, message, key=None):\n",
    "        if key is None:\n",
    "            key = KeyGenerator.generate_from_message(message, 128)\n",
    "\n",
    "        tokens = self.processor.encode(message).unsqueeze(0).to(self.device)\n",
    "        key = key.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.autoencoder(tokens, return_embeddings=True)\n",
    "            encrypted = self.crypto_layer.encrypt(embeddings, key)\n",
    "\n",
    "        return encrypted, key\n",
    "\n",
    "    def decrypt_message(self, encrypted, key):\n",
    "        key = key.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            decrypted = self.crypto_layer.decrypt(encrypted, key)\n",
    "            logits = self.autoencoder.decoder(decrypted)\n",
    "            tokens = torch.argmax(logits, dim=-1)\n",
    "            message = self.processor.decode(tokens[0])\n",
    "\n",
    "        return message\n",
    "\n",
    "    def eve_attack(self, encrypted):\n",
    "        with torch.no_grad():\n",
    "            logits = self.eve(encrypted)\n",
    "            tokens = torch.argmax(logits, dim=-1)\n",
    "            message = self.processor.decode(tokens[0])\n",
    "\n",
    "        return message\n",
    "\n",
    "    def evaluate(self, test_messages):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FINAL EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        bob_sims = []\n",
    "        eve_sims = []\n",
    "        key_sens = []\n",
    "\n",
    "        for msg in test_messages[:10]:\n",
    "            encrypted, correct_key = self.encrypt_message(msg)\n",
    "\n",
    "            bob_msg = self.decrypt_message(encrypted, correct_key)\n",
    "            eve_msg = self.eve_attack(encrypted)\n",
    "\n",
    "            wrong_sims = []\n",
    "            for _ in range(3):\n",
    "                wrong_key = KeyGenerator.generate_random(128)\n",
    "                wrong_msg = self.decrypt_message(encrypted, wrong_key)\n",
    "                wrong_sims.append(SequenceMatcher(None, msg, wrong_msg).ratio())\n",
    "\n",
    "            bob_sim = SequenceMatcher(None, msg, bob_msg).ratio()\n",
    "            eve_sim = SequenceMatcher(None, msg, eve_msg).ratio()\n",
    "            avg_wrong = np.mean(wrong_sims)\n",
    "\n",
    "            bob_sims.append(bob_sim)\n",
    "            eve_sims.append(eve_sim)\n",
    "            key_sens.append(1 - avg_wrong)\n",
    "\n",
    "            print(f\"\\nOriginal:  '{msg}'\")\n",
    "            print(f\"Bob:       '{bob_msg}' ({bob_sim*100:.1f}%)\")\n",
    "            print(f\"Eve:       '{eve_msg}' ({eve_sim*100:.1f}%)\")\n",
    "            print(f\"Wrong key: Avg {avg_wrong*100:.1f}% similarity\")\n",
    "\n",
    "        avg_bob = np.mean(bob_sims)\n",
    "        avg_eve = np.mean(eve_sims)\n",
    "        avg_key_sens = np.mean(key_sens)\n",
    "        security_ratio = avg_bob / max(avg_eve, 0.01)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FINAL METRICS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Bob Similarity:    {avg_bob*100:.1f}% {'✓' if avg_bob > 0.90 else '✗'}\")\n",
    "        print(f\"Eve Similarity:    {avg_eve*100:.1f}% {'✓' if avg_eve < 0.30 else '⚠️'}\")\n",
    "        print(f\"Key Sensitivity:   {avg_key_sens*100:.1f}% {'✓' if avg_key_sens > 0.50 else '⚠️'}\")\n",
    "        print(f\"Security Ratio:    {security_ratio:.2f}x {'✓' if security_ratio > 3.0 else '⚠️'}\")\n",
    "\n",
    "        if avg_bob > 0.90:\n",
    "            print(\"\\n✓ Bob: EXCELLENT decryption!\")\n",
    "        if avg_eve < 0.20:\n",
    "            print(\"✓ Eve: CANNOT break encryption!\")\n",
    "        if avg_key_sens > 0.60:\n",
    "            print(\"✓ Keys: Good sensitivity!\")\n",
    "\n",
    "        if avg_bob > 0.90 and security_ratio > 3:\n",
    "            print(\"\\n🎉 SUCCESS! System works well!\")\n",
    "\n",
    "        print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ============ Dataset ============\n",
    "LARGE_DATASET = [\n",
    "    \"Hello World!\", \"This is a test.\", \"Secret message here.\",\n",
    "    \"Encryption works!\", \"Neural crypto system.\", \"Testing ABC 123.\",\n",
    "    \"Quick brown fox.\", \"The lazy dog jumps.\",\n",
    "    \"Machine learning is powerful.\", \"Deep neural networks.\", \"Artificial intelligence evolves.\",\n",
    "    \"Natural language processing.\", \"Computer vision tasks.\", \"Reinforcement learning agent.\",\n",
    "    \"Gradient descent optimizer.\", \"Backpropagation algorithm.\", \"Model accuracy improves.\",\n",
    "    \"Training loss decreases.\", \"Validation metrics good.\", \"Test results excellent.\",\n",
    "    \"Good morning everyone.\", \"How are you today?\", \"See you tomorrow.\",\n",
    "    \"Thank you very much.\", \"Great job well done.\", \"Nice work keep going.\",\n",
    "    \"Data science project.\", \"Python programming fun.\", \"Code quality matters.\",\n",
    "    \"Documentation complete.\", \"Production ready now.\", \"System performance optimal.\",\n",
    "    \"Hi there!\", \"Goodbye!\", \"Yes indeed.\", \"No problem.\", \"Of course.\",\n",
    "    \"Absolutely right.\", \"Definitely true.\", \"Maybe later.\", \"Not now.\", \"Soon enough.\",\n",
    "    \"The sun rises early.\", \"Birds sing beautifully.\", \"Rivers flow downstream.\",\n",
    "    \"Mountains stand tall.\", \"Oceans are deep.\", \"Stars shine bright.\",\n",
    "    \"Music sounds wonderful.\", \"Books tell stories.\", \"Art inspires people.\",\n",
    "    \"Science explains nature.\", \"Math solves problems.\", \"History teaches lessons.\",\n",
    "]\n",
    "\n",
    "\n",
    "# ============ Main ============\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Dataset: {len(LARGE_DATASET)} messages\\n\")\n",
    "\n",
    "    processor = StringProcessor()\n",
    "    system = NeuralCryptoSystem(processor.vocab_size, embed_dim=128, device=device)\n",
    "\n",
    "    success = system.train_phase1_reconstruction(LARGE_DATASET, epochs=200)\n",
    "\n",
    "    if success:\n",
    "        system.train_phase2_with_encryption(LARGE_DATASET, epochs=100)\n",
    "        system.evaluate(LARGE_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8eae2c4a29cd4d559c24ac8644c6090c",
      "6874e27f3c5b4d3cae3f39f1b6173693",
      "e41af375d9d541b7842a5d1e525c686a",
      "babc9efced7d4538b16099bd70bbe4ca",
      "66f993b67156445c8858cc8d1e89808e",
      "ddd68cffcd2541e899d183cf98b01c34",
      "20f80ee32ab849c0a62e308b3eb7f60a",
      "a1e3914665d04704a030e76c8c58f2a0",
      "e951ce81745d4663b00b5d9fe319b6c8",
      "e3d94544e7cc4bfa9e7d4deba1622d0c",
      "a7cb97768c2c4981b75df2eb91075917",
      "c8ae513140c94104974cc438b79ae6c4",
      "7a18f87d557e42238fa00786b95ff4ce",
      "a1d71842610d442084c114f75dac45dc",
      "26e5f059f1244fe99a6b35a7d8773979",
      "b57068293c1d4b7fa619e62b5762eac0",
      "46eb37611e7145a1b98466d43460ece4",
      "fcb3e09ada8f43599c83bed0e306f0f0",
      "a2a8dc69580b499384b4a170e170a618",
      "64cd2f848e294795a0921290970040d9",
      "7995d5c72a494acdb3fd8da9d19a76dc",
      "201bfa2e7fbc4753a94906286fb3f6e8",
      "289297fa5eae4ddf8f81252d9432f49c",
      "96b97421c7a34a5cb5710d15949b9ee1",
      "dd6699cfa3474df69960ced68261aad0",
      "ceeafcd987f64e2eb82c051617a1fd9f",
      "4df43b7213924c33be1af578d171f57d",
      "694c33d62f934805be071d4cd396db32",
      "3554cbefdf6f4f9ca5033d6cc51beb62",
      "68ea570d87024a27a7ecbef450db2e15",
      "fd05be84cfbd47a78da51c2151e08c08",
      "430fdea6f0414f63b000ea20f09f6f77",
      "45094c24ef9c467f99a80ce4d52865e2",
      "e123ab6db1bc4ea9afd2ceae225aed4c",
      "6e03db9002b7443a8c79390730db21d7",
      "4eda87219b114c61b56607881b81f09f",
      "5e021932fb2a4f6cb3c9ccd3d0e9d6da",
      "ba6e35c8db8a4b36ad380f212397cad5",
      "13d491ea908b4ed0b493df7cb85ab31e",
      "20abbecba7aa46278d9e14845b2023eb",
      "1fe3108dd79f46409e28baa1ed51bdad",
      "01a3430f94f046c096058cd5c2584c64",
      "d1592704ed0f470a8b87b6ef0b606466",
      "6d6cab8bdc91492bb74e43f37b6bd1cc",
      "3cc7ee061e534547afb5c18969d94eec",
      "af2e3dc14e7f4ab1916bfb40e0a31ad7",
      "cf19e7f21b9441ecac9c86d7d9169840",
      "b0b31897d2e641f4b47d7f68c93bbe55",
      "50f4746cac5344958bae628111284a1b",
      "b774c31df38747e294cca188e61c1c4c",
      "875595eb936643d9b2ba8a3d9715b474",
      "1bbf70c4a76444b6bd36ea1b8b25fcb3",
      "29322ea5fb304cfba142a4e572f36137",
      "292669464ca54bd89d1658b667a9fe80",
      "967d0af8a73c483abfff29baa41d3491",
      "4a3f4ff55b8e41459fb7218567f2582b",
      "49d9e09994e740f1a03de27dd9815b85",
      "67c2c85a63ce4953843af2fc038ed81d",
      "0aae93c987a947ffa92a61151535d85f",
      "80e7ad20f9b6401aa8417a0f5d84c31e",
      "89ade37192a4407f9d360636563e1d82",
      "bfc5e8a094d14391809d6b7a3b006263",
      "5d82a69bccf341e5a01da7401c740800",
      "71b42ebbca90476b9768853fa923d83a",
      "18e1d70953dc45cdbefd8b9f29b5cfca",
      "4eee05526529460e8b7499cdc1b3df9e",
      "75a0c24128d7470b9f1c01cdeab4123f",
      "ced451e65cfb474d9e76569922cac7cd",
      "142462b6ce644f199b425f95cc8eaa14",
      "0d4da58717de4ce0aaf3c2f867fa545b",
      "959a9f94b40043218f180eae9551a409",
      "cd995add548c40349bba59bf741b0b8d",
      "acd5189144e64ba69b6b857a36b9fd3c",
      "9538eba457ec4bb18ae9808f14e937c7",
      "c63001a005f64d61872a87ee42ad595b",
      "32a1993ae98d457994ad34f6c3246be3",
      "0e5b000f1787496b8ae5a3a96bde3b4c"
     ]
    },
    "id": "fYo663z2vngS",
    "outputId": "53789ff9-552f-4976-bbee-bf3a3fb48939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Loading dataset: imdb\n",
      "Max samples: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eae2c4a29cd4d559c24ac8644c6090c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ae513140c94104974cc438b79ae6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289297fa5eae4ddf8f81252d9432f49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e123ab6db1bc4ea9afd2ceae225aed4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/unsupervised-00000-of-00001.p(…):   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc7ee061e534547afb5c18969d94eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3f4ff55b8e41459fb7218567f2582b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a0c24128d7470b9f1c01cdeab4123f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 10000 movie reviews from IMDB\n",
      "✓ After cleaning: 10000 valid texts\n",
      "\n",
      "Final dataset size: 10000 messages\n",
      "\n",
      "\n",
      "======================================================================\n",
      "PHASE 1: RECONSTRUCTION TRAINING (NO ENCRYPTION)\n",
      "======================================================================\n",
      "Epoch   0 | Loss: 0.1110 | Acc: 98.8%\n",
      "Epoch  20 | Loss: 0.0000 | Acc: 100.0%\n",
      "Epoch  40 | Loss: 0.0000 | Acc: 100.0%\n",
      "Epoch  60 | Loss: 0.0000 | Acc: 100.0%\n",
      "Epoch  80 | Loss: 0.0000 | Acc: 100.0%\n",
      "\n",
      "✓ Phase 1 Complete! Accuracy: 100.0%\n",
      "\n",
      "======================================================================\n",
      "PHASE 2: ENCRYPTION TRAINING\n",
      "======================================================================\n",
      "Epoch   0 | Bob: 19.827 (6.1%) | Eve: 4.310 (2.1%) | Ratio: 0.22x\n",
      "Epoch  10 | Bob: 0.017 (99.4%) | Eve: 2.882 (56.8%) | Ratio: 172.68x\n",
      "Epoch  20 | Bob: 0.015 (99.6%) | Eve: 2.134 (73.2%) | Ratio: 144.99x\n",
      "Epoch  30 | Bob: 0.002 (100.0%) | Eve: 1.847 (71.9%) | Ratio: 962.68x\n",
      "Epoch  40 | Bob: 0.003 (100.0%) | Eve: 1.578 (75.4%) | Ratio: 456.35x\n",
      "Epoch  50 | Bob: 0.003 (99.8%) | Eve: 1.161 (83.6%) | Ratio: 374.07x\n",
      "Epoch  60 | Bob: 0.490 (93.4%) | Eve: 1.349 (75.6%) | Ratio: 2.75x\n",
      "Epoch  70 | Bob: 0.032 (99.2%) | Eve: 1.623 (66.4%) | Ratio: 49.96x\n",
      "Epoch  80 | Bob: 0.023 (99.2%) | Eve: 2.148 (44.9%) | Ratio: 91.57x\n",
      "Epoch  90 | Bob: 0.037 (98.4%) | Eve: 2.696 (34.6%) | Ratio: 73.49x\n",
      "\n",
      "✓ Phase 2 Complete!\n",
      "\n",
      "======================================================================\n",
      "FINAL EVALUATION\n",
      "======================================================================\n",
      "\n",
      "Original:  '....as to the level of wit on which this comedy op...'\n",
      "Bob:       '....as to the level of wit on which this comedy op...' (99.2%)\n",
      "Eve:       'oooooooo  or  o o loo  o ooor o  or oh soooo oo os...' (10.9%)\n",
      "Wrong key: Avg 8.3% similarity\n",
      "\n",
      "Original:  '...at least during its first half. If it had start...'\n",
      "Bob:       '...at least during its first half. If it had start...' (99.2%)\n",
      "Eve:       '                         o               o        ...' (20.3%)\n",
      "Wrong key: Avg 15.6% similarity\n",
      "\n",
      "Original:  'Bad. Bad. Bad. Bad. Bad. What else can I say. Kate...'\n",
      "Bob:       'Bad. Bad. Bad. Bad. Bad. What else can I say. Kate...' (100.0%)\n",
      "Eve:       'nnmindnmsmdsnnaansnannmiln nan admmmnsanmdnnneatse...' (20.5%)\n",
      "Wrong key: Avg 11.3% similarity\n",
      "\n",
      "Original:  'Wanda Nevada is a pubescent fantasy movie using ci...'\n",
      "Bob:       'Wanda Nevada is a pubescent fantasy movie using ci...' (94.5%)\n",
      "Eve:       'm id msis ii  s   nube sens iini ti i s i u  n  ui...' (32.8%)\n",
      "Wrong key: Avg 7.7% similarity\n",
      "\n",
      "Original:  'The plot was quite interesting, with the Russian r...'\n",
      "Bob:       'The plot was quite interesting, with the Russian r...' (99.2%)\n",
      "Eve:       'r eesireeeeeeereeeieseeleeeeeeeeeei eeeeeeeeeeeeee...' (18.8%)\n",
      "Wrong key: Avg 4.2% similarity\n",
      "\n",
      "Original:  'I have read the book a couple of times and this mo...'\n",
      "Bob:       'I have read the book a couple of times and this mo...' (99.2%)\n",
      "Eve:       'r hr r rhro ohe hoohir ooollrrorooilrs rro ihrs lo...' (40.6%)\n",
      "Wrong key: Avg 4.7% similarity\n",
      "\n",
      "Original:  'This film is about British prisoners of war from t...'\n",
      "Bob:       'This film is about British prisoners of war from t...' (99.2%)\n",
      "Eve:       'slsesissessnssseeseeesssn esesessseessiseseeesssss...' (15.6%)\n",
      "Wrong key: Avg 10.3% similarity\n",
      "\n",
      "Original:  'Tobe Hooper is quite possibly the biggest fluke th...'\n",
      "Bob:       'Tobe Hooper is quite possibly the biggest fluke th...' (99.2%)\n",
      "Eve:       'sose  ooper  s et oe soss tl  ohe tiiieso llsoe oh...' (65.6%)\n",
      "Wrong key: Avg 6.8% similarity\n",
      "\n",
      "Original:  'Yeah, right.<br /><br />I spent the first hour wai...'\n",
      "Bob:       'Yeah, right. br    br   I spent the first hour wai...' (89.8%)\n",
      "Eve:       'otihitniihttmtnotootnoo nttaentothiofinttohotnttni...' (39.1%)\n",
      "Wrong key: Avg 8.3% similarity\n",
      "\n",
      "Original:  'This movie isn't very good. It's boring, and not m...'\n",
      "Bob:       'This movie isn't very good. It's boring, and not m...' (100.0%)\n",
      "Eve:       ' oh  i  ao a     a o  a h       iie    a o l  hl  ...' (28.3%)\n",
      "Wrong key: Avg 12.0% similarity\n",
      "\n",
      "======================================================================\n",
      "FINAL METRICS\n",
      "======================================================================\n",
      "Bob Similarity:    98.0% ✓\n",
      "Eve Similarity:    29.3% ✓\n",
      "Key Sensitivity:   91.1% ✓\n",
      "Security Ratio:    3.35x ✓\n",
      "\n",
      "✓ Bob: EXCELLENT decryption!\n",
      "✓ Keys: Good sensitivity!\n",
      "\n",
      "🎉 SUCCESS! System works well!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NEURAL CRYPTO SYSTEM WITH LARGE DATASET SUPPORT\n",
    "Supports multiple open-source datasets from Hugging Face\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from difflib import SequenceMatcher\n",
    "import hashlib\n",
    "\n",
    "# ============ String Processor ============\n",
    "class StringProcessor:\n",
    "    def __init__(self, max_len=64):\n",
    "        self.max_len = max_len\n",
    "        chars = ' abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,!?-\\'\"'\n",
    "        self.vocab = {c: i for i, c in enumerate(chars)}\n",
    "        self.vocab['<PAD>'] = len(self.vocab)\n",
    "        self.vocab['<END>'] = len(self.vocab)\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def encode(self, text):\n",
    "        ids = [self.vocab.get(c, 0) for c in text[:self.max_len-1]]\n",
    "        ids.append(self.vocab['<END>'])\n",
    "        while len(ids) < self.max_len:\n",
    "            ids.append(self.vocab['<PAD>'])\n",
    "        return torch.LongTensor(ids)\n",
    "\n",
    "    def decode(self, ids):\n",
    "        chars = []\n",
    "        for i in ids:\n",
    "            c = self.inv_vocab.get(int(i), '')\n",
    "            if c == '<END>':\n",
    "                break\n",
    "            if c != '<PAD>':\n",
    "                chars.append(c)\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def batch_encode(self, texts):\n",
    "        return torch.stack([self.encode(t) for t in texts])\n",
    "\n",
    "\n",
    "# ============ Dataset Loader ============\n",
    "class DatasetLoader:\n",
    "    \"\"\"Load various open-source datasets\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_dataset(dataset_name='imdb', max_samples=10000, max_len=64):\n",
    "        \"\"\"\n",
    "        Load dataset from various sources\n",
    "\n",
    "        Available datasets:\n",
    "        - 'imdb': Movie reviews (Hugging Face)\n",
    "        - 'ag_news': News articles (Hugging Face)\n",
    "        - 'yelp': Restaurant reviews (Hugging Face)\n",
    "        - 'sst2': Sentiment analysis (Hugging Face)\n",
    "        - 'tweets': Twitter sentiment (Hugging Face)\n",
    "        - 'wikitext': Wikipedia articles (Hugging Face)\n",
    "        - 'news': News headlines (Hugging Face)\n",
    "        \"\"\"\n",
    "        print(f\"\\nLoading dataset: {dataset_name}\")\n",
    "        print(f\"Max samples: {max_samples}\")\n",
    "\n",
    "        try:\n",
    "            from datasets import load_dataset as hf_load_dataset\n",
    "\n",
    "            if dataset_name == 'imdb':\n",
    "                # Movie reviews - balanced positive/negative\n",
    "                dataset = hf_load_dataset('imdb', split='train')\n",
    "                texts = [item['text'][:max_len] for item in dataset.select(range(min(max_samples, len(dataset))))]\n",
    "                print(f\"✓ Loaded {len(texts)} movie reviews from IMDB\")\n",
    "\n",
    "            elif dataset_name == 'ag_news':\n",
    "                # News articles with categories\n",
    "                dataset = hf_load_dataset('ag_news', split='train')\n",
    "                texts = [item['text'][:max_len] for item in dataset.select(range(min(max_samples, len(dataset))))]\n",
    "                print(f\"✓ Loaded {len(texts)} news articles from AG News\")\n",
    "\n",
    "            elif dataset_name == 'yelp':\n",
    "                # Restaurant reviews\n",
    "                dataset = hf_load_dataset('yelp_review_full', split='train')\n",
    "                texts = [item['text'][:max_len] for item in dataset.select(range(min(max_samples, len(dataset))))]\n",
    "                print(f\"✓ Loaded {len(texts)} restaurant reviews from Yelp\")\n",
    "\n",
    "            elif dataset_name == 'sst2':\n",
    "                # Stanford Sentiment Treebank\n",
    "                dataset = hf_load_dataset('glue', 'sst2', split='train')\n",
    "                texts = [item['sentence'][:max_len] for item in dataset.select(range(min(max_samples, len(dataset))))]\n",
    "                print(f\"✓ Loaded {len(texts)} sentences from SST-2\")\n",
    "\n",
    "            elif dataset_name == 'tweets':\n",
    "                # Twitter sentiment\n",
    "                dataset = hf_load_dataset('tweet_eval', 'sentiment', split='train')\n",
    "                texts = [item['text'][:max_len] for item in dataset.select(range(min(max_samples, len(dataset))))]\n",
    "                print(f\"✓ Loaded {len(texts)} tweets\")\n",
    "\n",
    "            elif dataset_name == 'wikitext':\n",
    "                # Wikipedia articles\n",
    "                dataset = hf_load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "                # Filter out empty lines and split into sentences\n",
    "                texts = []\n",
    "                for item in dataset:\n",
    "                    text = item['text'].strip()\n",
    "                    if text and len(text) > 10:\n",
    "                        # Split into sentences\n",
    "                        sentences = text.split('. ')\n",
    "                        for sent in sentences:\n",
    "                            if 10 <= len(sent) <= max_len:\n",
    "                                texts.append(sent)\n",
    "                                if len(texts) >= max_samples:\n",
    "                                    break\n",
    "                    if len(texts) >= max_samples:\n",
    "                        break\n",
    "                print(f\"✓ Loaded {len(texts)} Wikipedia sentences\")\n",
    "\n",
    "            elif dataset_name == 'news':\n",
    "                # News headlines\n",
    "                dataset = hf_load_dataset('Fraser/news-category-dataset', split='train')\n",
    "                texts = [item['headline'][:max_len] for item in dataset.select(range(min(max_samples, len(dataset))))]\n",
    "                print(f\"✓ Loaded {len(texts)} news headlines\")\n",
    "\n",
    "            else:\n",
    "                print(f\"⚠️ Unknown dataset: {dataset_name}\")\n",
    "                return DatasetLoader.get_default_dataset()\n",
    "\n",
    "            # Clean and filter texts\n",
    "            cleaned_texts = []\n",
    "            for text in texts:\n",
    "                text = text.strip()\n",
    "                if 5 <= len(text) <= max_len:  # Reasonable length\n",
    "                    cleaned_texts.append(text)\n",
    "\n",
    "            print(f\"✓ After cleaning: {len(cleaned_texts)} valid texts\")\n",
    "            return np.array(cleaned_texts)\n",
    "\n",
    "        except ImportError:\n",
    "            print(\"⚠️ Hugging Face datasets not installed!\")\n",
    "            print(\"Installing: pip install datasets\")\n",
    "            print(\"\\nUsing default dataset instead...\\n\")\n",
    "            return DatasetLoader.get_default_dataset()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error loading dataset: {e}\")\n",
    "            print(\"Using default dataset instead...\\n\")\n",
    "            return DatasetLoader.get_default_dataset()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_dataset():\n",
    "        \"\"\"Fallback to default dataset if loading fails\"\"\"\n",
    "        return np.array([\n",
    "            \"Hello World!\", \"This is a test.\", \"Secret message here.\",\n",
    "            \"Encryption works!\", \"Neural crypto system.\", \"Testing ABC 123.\",\n",
    "            \"Quick brown fox.\", \"The lazy dog jumps.\",\n",
    "            \"Machine learning is powerful.\", \"Deep neural networks.\",\n",
    "            \"Artificial intelligence evolves.\", \"Natural language processing.\",\n",
    "            \"Computer vision tasks.\", \"Reinforcement learning agent.\",\n",
    "            \"Gradient descent optimizer.\", \"Backpropagation algorithm.\",\n",
    "            \"Model accuracy improves.\", \"Training loss decreases.\",\n",
    "            \"Validation metrics good.\", \"Test results excellent.\",\n",
    "            \"Good morning everyone.\", \"How are you today?\",\n",
    "            \"See you tomorrow.\", \"Thank you very much.\",\n",
    "            \"Great job well done.\", \"Nice work keep going.\",\n",
    "            \"Data science project.\", \"Python programming fun.\",\n",
    "            \"Code quality matters.\", \"Documentation complete.\",\n",
    "            \"Production ready now.\", \"System performance optimal.\",\n",
    "        ])\n",
    "\n",
    "    @staticmethod\n",
    "    def download_text_file(url, max_samples=10000, max_len=64):\n",
    "        \"\"\"Download text file from URL and extract sentences\"\"\"\n",
    "        try:\n",
    "            import requests\n",
    "            print(f\"\\nDownloading from: {url}\")\n",
    "            response = requests.get(url)\n",
    "            text = response.text\n",
    "\n",
    "            # Split into lines/sentences\n",
    "            lines = text.split('\\n')\n",
    "            texts = []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if 5 <= len(line) <= max_len:\n",
    "                    texts.append(line)\n",
    "                    if len(texts) >= max_samples:\n",
    "                        break\n",
    "\n",
    "            print(f\"✓ Loaded {len(texts)} lines from URL\")\n",
    "            return np.array(texts)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error downloading: {e}\")\n",
    "            return DatasetLoader.get_default_dataset()\n",
    "\n",
    "\n",
    "# ============ Autoencoder ============\n",
    "class CryptoAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=vocab_size-2)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, tokens, return_embeddings=False):\n",
    "        emb = self.embedding(tokens)\n",
    "        enc = self.encoder(emb)\n",
    "\n",
    "        if return_embeddings:\n",
    "            return enc\n",
    "\n",
    "        logits = self.decoder(enc)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ============ Key Generator ============\n",
    "class KeyGenerator:\n",
    "    @staticmethod\n",
    "    def generate_from_message(message, key_size=128):\n",
    "        msg_hash = hashlib.sha256(message.encode()).hexdigest()\n",
    "        seed = int(msg_hash[:8], 16)\n",
    "        np.random.seed(seed)\n",
    "        key = torch.FloatTensor(np.random.randn(key_size))\n",
    "        return key\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_random(key_size=128):\n",
    "        return torch.randn(key_size)\n",
    "\n",
    "\n",
    "# ============ Key-Dependent Encryption ============\n",
    "class KeyDependentEncryption(nn.Module):\n",
    "    def __init__(self, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.key_transform = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(embed_dim * 2, embed_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.encrypt_mix = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, embed_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def encrypt(self, embeddings, key):\n",
    "        if len(key.shape) == 1:\n",
    "            key = key.unsqueeze(0).expand(embeddings.size(0), -1)\n",
    "\n",
    "        key_features = self.key_transform(key)\n",
    "        key_expanded = key_features.unsqueeze(1).expand(-1, embeddings.size(1), -1)\n",
    "\n",
    "        combined = torch.cat([embeddings, key_expanded], dim=-1)\n",
    "        encrypted = self.encrypt_mix(combined)\n",
    "        encrypted = encrypted * (1 + key_expanded * 2)\n",
    "\n",
    "        return encrypted\n",
    "\n",
    "    def decrypt(self, encrypted, key):\n",
    "        if len(key.shape) == 1:\n",
    "            key = key.unsqueeze(0).expand(encrypted.size(0), -1)\n",
    "\n",
    "        key_features = self.key_transform(key)\n",
    "        key_expanded = key_features.unsqueeze(1).expand(-1, encrypted.size(1), -1)\n",
    "        decrypted = encrypted / (1 + key_expanded * 2 + 1e-8)\n",
    "\n",
    "        return decrypted\n",
    "\n",
    "\n",
    "# ============ Eve Attacker ============\n",
    "class EveAttacker(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.attack_network = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 2),\n",
    "            nn.LayerNorm(embed_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(embed_dim * 2, embed_dim * 2),\n",
    "            nn.LayerNorm(embed_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(embed_dim * 2, embed_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim, vocab_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, encrypted_embeddings):\n",
    "        return self.attack_network(encrypted_embeddings)\n",
    "\n",
    "\n",
    "# ============ Neural Crypto System ============\n",
    "class NeuralCryptoSystem:\n",
    "    def __init__(self, vocab_size, embed_dim=128, device='cuda'):\n",
    "        self.device = device\n",
    "        self.processor = StringProcessor()\n",
    "\n",
    "        self.autoencoder = CryptoAutoencoder(vocab_size, embed_dim).to(device)\n",
    "        self.crypto_layer = KeyDependentEncryption(embed_dim).to(device)\n",
    "        self.eve = EveAttacker(vocab_size, embed_dim).to(device)\n",
    "\n",
    "        self.opt_main = optim.Adam(\n",
    "            list(self.autoencoder.parameters()) + list(self.crypto_layer.parameters()),\n",
    "            lr=0.001\n",
    "        )\n",
    "        self.opt_eve = optim.Adam(self.eve.parameters(), lr=0.0005)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train_phase1_reconstruction(self, messages, epochs=200, batch_size=32):\n",
    "        \"\"\"Train with mini-batches for large datasets\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PHASE 1: RECONSTRUCTION TRAINING (NO ENCRYPTION)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(len(messages))\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "            num_batches = 0\n",
    "\n",
    "            # Mini-batch training\n",
    "            for i in range(0, len(messages), batch_size):\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                batch_msgs = messages[batch_indices]\n",
    "\n",
    "                tokens = self.processor.batch_encode(batch_msgs).to(self.device)\n",
    "\n",
    "                self.opt_main.zero_grad()\n",
    "                logits = self.autoencoder(tokens)\n",
    "\n",
    "                loss = self.criterion(logits.view(-1, logits.size(-1)), tokens.view(-1))\n",
    "                loss.backward()\n",
    "                self.opt_main.step()\n",
    "\n",
    "                pred = torch.argmax(logits, dim=-1)\n",
    "                acc = (pred == tokens).float().mean().item()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_acc += acc\n",
    "                num_batches += 1\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                avg_loss = total_loss / num_batches\n",
    "                avg_acc = total_acc / num_batches\n",
    "                print(f\"Epoch {epoch:3d} | Loss: {avg_loss:.4f} | Acc: {avg_acc*100:.1f}%\")\n",
    "\n",
    "        final_acc = total_acc / num_batches\n",
    "        print(f\"\\n✓ Phase 1 Complete! Accuracy: {final_acc*100:.1f}%\")\n",
    "        return final_acc > 0.95\n",
    "\n",
    "    def train_phase2_with_encryption(self, messages, epochs=100, batch_size=8):\n",
    "        \"\"\"Phase 2 with large dataset support\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PHASE 2: ENCRYPTION TRAINING\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Sample random batch\n",
    "            batch_indices = np.random.choice(len(messages), min(batch_size, len(messages)), replace=False)\n",
    "            batch_msgs = messages[batch_indices]\n",
    "            tokens = self.processor.batch_encode(batch_msgs).to(self.device)\n",
    "\n",
    "            keys = torch.stack([KeyGenerator.generate_random(128) for _ in batch_msgs]).to(self.device)\n",
    "\n",
    "            # Train Alice+Bob\n",
    "            self.opt_main.zero_grad()\n",
    "            embeddings = self.autoencoder(tokens, return_embeddings=True)\n",
    "            encrypted = self.crypto_layer.encrypt(embeddings, keys)\n",
    "            decrypted = self.crypto_layer.decrypt(encrypted, keys)\n",
    "            logits = self.autoencoder.decoder(decrypted)\n",
    "\n",
    "            loss_reconstruction = self.criterion(logits.view(-1, logits.size(-1)), tokens.view(-1))\n",
    "            loss_reconstruction.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.autoencoder.parameters(), 1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(self.crypto_layer.parameters(), 1.0)\n",
    "            self.opt_main.step()\n",
    "\n",
    "            # Train Eve\n",
    "            self.opt_eve.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                embeddings = self.autoencoder(tokens, return_embeddings=True)\n",
    "                encrypted = self.crypto_layer.encrypt(embeddings, keys)\n",
    "\n",
    "            eve_logits = self.eve(encrypted)\n",
    "            loss_eve = self.criterion(eve_logits.view(-1, eve_logits.size(-1)), tokens.view(-1))\n",
    "            loss_eve.backward()\n",
    "            self.opt_eve.step()\n",
    "\n",
    "            # Adversarial training\n",
    "            if epoch > 30:\n",
    "                self.opt_main.zero_grad()\n",
    "                embeddings = self.autoencoder(tokens, return_embeddings=True)\n",
    "                encrypted = self.crypto_layer.encrypt(embeddings, keys)\n",
    "                eve_attack = self.eve(encrypted)\n",
    "\n",
    "                loss_adversarial = -self.criterion(eve_attack.view(-1, eve_attack.size(-1)), tokens.view(-1))\n",
    "                (loss_adversarial * 0.1).backward()\n",
    "                self.opt_main.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                bob_pred = torch.argmax(logits, dim=-1)\n",
    "                eve_pred = torch.argmax(eve_logits, dim=-1)\n",
    "\n",
    "                bob_acc = (bob_pred == tokens).float().mean().item()\n",
    "                eve_acc = (eve_pred == tokens).float().mean().item()\n",
    "                ratio = loss_eve.item() / (loss_reconstruction.item() + 1e-8)\n",
    "\n",
    "                print(f\"Epoch {epoch:3d} | Bob: {loss_reconstruction.item():.3f} ({bob_acc*100:.1f}%) | \"\n",
    "                      f\"Eve: {loss_eve.item():.3f} ({eve_acc*100:.1f}%) | Ratio: {ratio:.2f}x\")\n",
    "\n",
    "        print(\"\\n✓ Phase 2 Complete!\")\n",
    "\n",
    "    def encrypt_message(self, message, key=None):\n",
    "        if key is None:\n",
    "            key = KeyGenerator.generate_from_message(message, 128)\n",
    "\n",
    "        tokens = self.processor.encode(message).unsqueeze(0).to(self.device)\n",
    "        key = key.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.autoencoder(tokens, return_embeddings=True)\n",
    "            encrypted = self.crypto_layer.encrypt(embeddings, key)\n",
    "\n",
    "        return encrypted, key\n",
    "\n",
    "    def decrypt_message(self, encrypted, key):\n",
    "        key = key.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            decrypted = self.crypto_layer.decrypt(encrypted, key)\n",
    "            logits = self.autoencoder.decoder(decrypted)\n",
    "            tokens = torch.argmax(logits, dim=-1)\n",
    "            message = self.processor.decode(tokens[0])\n",
    "\n",
    "        return message\n",
    "\n",
    "    def eve_attack(self, encrypted):\n",
    "        with torch.no_grad():\n",
    "            logits = self.eve(encrypted)\n",
    "            tokens = torch.argmax(logits, dim=-1)\n",
    "            message = self.processor.decode(tokens[0])\n",
    "\n",
    "        return message\n",
    "\n",
    "    def evaluate(self, test_messages):\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FINAL EVALUATION\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        bob_sims = []\n",
    "        eve_sims = []\n",
    "        key_sens = []\n",
    "\n",
    "        # Evaluate on random sample\n",
    "        eval_msgs = np.random.choice(test_messages, min(10, len(test_messages)), replace=False)\n",
    "\n",
    "        for msg in eval_msgs:\n",
    "            encrypted, correct_key = self.encrypt_message(msg)\n",
    "\n",
    "            bob_msg = self.decrypt_message(encrypted, correct_key)\n",
    "            eve_msg = self.eve_attack(encrypted)\n",
    "\n",
    "            wrong_sims = []\n",
    "            for _ in range(3):\n",
    "                wrong_key = KeyGenerator.generate_random(128)\n",
    "                wrong_msg = self.decrypt_message(encrypted, wrong_key)\n",
    "                wrong_sims.append(SequenceMatcher(None, msg, wrong_msg).ratio())\n",
    "\n",
    "            bob_sim = SequenceMatcher(None, msg, bob_msg).ratio()\n",
    "            eve_sim = SequenceMatcher(None, msg, eve_msg).ratio()\n",
    "            avg_wrong = np.mean(wrong_sims)\n",
    "\n",
    "            bob_sims.append(bob_sim)\n",
    "            eve_sims.append(eve_sim)\n",
    "            key_sens.append(1 - avg_wrong)\n",
    "\n",
    "            print(f\"\\nOriginal:  '{msg[:50]}...'\")\n",
    "            print(f\"Bob:       '{bob_msg[:50]}...' ({bob_sim*100:.1f}%)\")\n",
    "            print(f\"Eve:       '{eve_msg[:50]}...' ({eve_sim*100:.1f}%)\")\n",
    "            print(f\"Wrong key: Avg {avg_wrong*100:.1f}% similarity\")\n",
    "\n",
    "        avg_bob = np.mean(bob_sims)\n",
    "        avg_eve = np.mean(eve_sims)\n",
    "        avg_key_sens = np.mean(key_sens)\n",
    "        security_ratio = avg_bob / max(avg_eve, 0.01)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FINAL METRICS\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Bob Similarity:    {avg_bob*100:.1f}% {'✓' if avg_bob > 0.90 else '✗'}\")\n",
    "        print(f\"Eve Similarity:    {avg_eve*100:.1f}% {'✓' if avg_eve < 0.30 else '⚠️'}\")\n",
    "        print(f\"Key Sensitivity:   {avg_key_sens*100:.1f}% {'✓' if avg_key_sens > 0.50 else '⚠️'}\")\n",
    "        print(f\"Security Ratio:    {security_ratio:.2f}x {'✓' if security_ratio > 3.0 else '⚠️'}\")\n",
    "\n",
    "        if avg_bob > 0.90:\n",
    "            print(\"\\n✓ Bob: EXCELLENT decryption!\")\n",
    "        if avg_eve < 0.20:\n",
    "            print(\"✓ Eve: CANNOT break encryption!\")\n",
    "        if avg_key_sens > 0.60:\n",
    "            print(\"✓ Keys: Good sensitivity!\")\n",
    "\n",
    "        if avg_bob > 0.90 and security_ratio > 3:\n",
    "            print(\"\\n🎉 SUCCESS! System works well!\")\n",
    "\n",
    "        print(\"=\"*70)\n",
    "\n",
    "\n",
    "# ============ Main ============\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # ============ CHOOSE YOUR DATASET ============\n",
    "    # Option 1: Load from Hugging Face (recommended)\n",
    "    DATASET = DatasetLoader.load_dataset(\n",
    "        dataset_name='imdb',      # Options: 'imdb', 'ag_news', 'yelp', 'sst2', 'tweets', 'wikitext', 'news'\n",
    "        max_samples=10000,         # Number of samples to load\n",
    "        max_len=64                 # Maximum text length\n",
    "    )\n",
    "\n",
    "    # Option 2: Load from URL (uncomment to use)\n",
    "    # DATASET = DatasetLoader.download_text_file(\n",
    "    #     url='https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt',\n",
    "    #     max_samples=10000,\n",
    "    #     max_len=64\n",
    "    # )\n",
    "\n",
    "    # Option 3: Use default dataset\n",
    "    # DATASET = DatasetLoader.get_default_dataset()\n",
    "\n",
    "    print(f\"\\nFinal dataset size: {len(DATASET)} messages\\n\")\n",
    "\n",
    "    # Initialize system\n",
    "    processor = StringProcessor()\n",
    "    system = NeuralCryptoSystem(processor.vocab_size, embed_dim=128, device=device)\n",
    "\n",
    "    # Train\n",
    "    success = system.train_phase1_reconstruction(DATASET, epochs=100, batch_size=32)\n",
    "\n",
    "    if success:\n",
    "        system.train_phase2_with_encryption(DATASET, epochs=100, batch_size=8)\n",
    "        system.evaluate(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
