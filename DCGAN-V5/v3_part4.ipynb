{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0EbKJqNwFROl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "class ImprovedCryptoTrainer:\n",
        "    \"\"\"Enhanced training with focus on security\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=98, embed_dim=128, alice_layers=4,\n",
        "                 bob_layers=4, eve_layers=6, max_len=128,\n",
        "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.max_len = max_len\n",
        "\n",
        "        from v3_part1 import StringProcessor, KeyManager\n",
        "        from v3_part2_alice import AliceEncryptor\n",
        "        from v3_part3_bob_eve import BobDecryptor, EveAttacker\n",
        "\n",
        "        self.processor = StringProcessor(max_length=max_len)\n",
        "        self.key_manager = KeyManager(device=device)\n",
        "\n",
        "        self.alice_wrapper = AliceEncryptor(vocab_size, embed_dim, alice_layers, max_len, device)\n",
        "        self.bob_wrapper = BobDecryptor(vocab_size, embed_dim, bob_layers, max_len, device)\n",
        "        self.eve_wrapper = EveAttacker(vocab_size, embed_dim, eve_layers, max_len, device)\n",
        "\n",
        "        self.alice = self.alice_wrapper.alice\n",
        "        self.bob = self.bob_wrapper.bob\n",
        "        self.eve = self.eve_wrapper.eve\n",
        "\n",
        "        # Optimizers with better settings\n",
        "        self.opt_alice = optim.AdamW(self.alice.parameters(), lr=0.001, weight_decay=0.01)\n",
        "        self.opt_bob = optim.AdamW(self.bob.parameters(), lr=0.001, weight_decay=0.01)\n",
        "        self.opt_eve = optim.Adam(self.eve.parameters(), lr=0.0003)\n",
        "\n",
        "        # Schedulers\n",
        "        self.sched_alice = optim.lr_scheduler.CosineAnnealingWarmRestarts(self.opt_alice, T_0=20, T_mult=2)\n",
        "        self.sched_bob = optim.lr_scheduler.CosineAnnealingWarmRestarts(self.opt_bob, T_0=20, T_mult=2)\n",
        "        self.sched_eve = optim.lr_scheduler.ReduceLROnPlateau(self.opt_eve, patience=10, factor=0.5)\n",
        "\n",
        "        # Loss\n",
        "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "        self.current_stage = 1\n",
        "\n",
        "    def train_stage1_reconstruction(self, messages, num_epochs=150):\n",
        "        \"\"\"Stage 1: Perfect reconstruction\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"STAGE 1: TRAINING ALICE+BOB FOR RECONSTRUCTION\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        history = {'loss': [], 'accuracy': []}\n",
        "        best_acc = 0\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            batch = np.random.choice(messages, size=min(8, len(messages)), replace=False).tolist()\n",
        "            tokens = self.processor.batch_encode(batch).to(self.device)\n",
        "            keys = self.key_manager.generate_keys_for_batch(len(batch), tokens.size(1))\n",
        "\n",
        "            self.opt_alice.zero_grad()\n",
        "            self.opt_bob.zero_grad()\n",
        "\n",
        "            self.alice.train()\n",
        "            self.bob.train()\n",
        "\n",
        "            encrypted = self.alice(tokens, keys['key_tensors'])\n",
        "            logits = self.bob(encrypted, keys['key_tensors'])\n",
        "\n",
        "            loss = self.ce_loss(logits.reshape(-1, logits.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.alice.parameters(), 0.5)\n",
        "            torch.nn.utils.clip_grad_norm_(self.bob.parameters(), 0.5)\n",
        "\n",
        "            self.opt_alice.step()\n",
        "            self.opt_bob.step()\n",
        "            self.sched_alice.step()\n",
        "            self.sched_bob.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred_tokens = torch.argmax(logits, dim=-1)\n",
        "                mask = tokens != 0\n",
        "                correct = ((pred_tokens == tokens) & mask).sum().item()\n",
        "                total = mask.sum().item()\n",
        "                accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "            history['loss'].append(loss.item())\n",
        "            history['accuracy'].append(accuracy)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | Accuracy: {accuracy*100:.1f}%\")\n",
        "\n",
        "            # Early stopping\n",
        "            if accuracy > best_acc:\n",
        "                best_acc = accuracy\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if best_acc > 0.95 and patience_counter > 20:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "        final_acc = np.mean(history['accuracy'][-10:])\n",
        "        print(f\"\\nStage 1 Complete! Final accuracy: {final_acc*100:.1f}%\")\n",
        "\n",
        "        if final_acc > 0.90:\n",
        "            print(\"SUCCESS: Bob can decrypt accurately!\")\n",
        "            self.current_stage = 2\n",
        "        else:\n",
        "            print(\"WARNING: Bob accuracy < 90%\")\n",
        "\n",
        "        return history\n",
        "\n",
        "    def train_stage2_adversary(self, messages, num_epochs=50):\n",
        "        \"\"\"Stage 2: Train Eve\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"STAGE 2: TRAINING EVE (ADVERSARY)\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        for param in self.alice.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.bob.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        history = {'loss': [], 'accuracy': []}\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            batch = np.random.choice(messages, size=min(6, len(messages)), replace=False).tolist()\n",
        "            tokens = self.processor.batch_encode(batch).to(self.device)\n",
        "            keys = self.key_manager.generate_keys_for_batch(len(batch), tokens.size(1))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                encrypted = self.alice(tokens, keys['key_tensors'])\n",
        "\n",
        "            self.opt_eve.zero_grad()\n",
        "            self.eve.train()\n",
        "\n",
        "            eve_logits = self.eve(encrypted)\n",
        "            loss = self.ce_loss(eve_logits.reshape(-1, eve_logits.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.eve.parameters(), 1.0)\n",
        "            self.opt_eve.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                pred_tokens = torch.argmax(eve_logits, dim=-1)\n",
        "                mask = tokens != 0\n",
        "                correct = ((pred_tokens == tokens) & mask).sum().item()\n",
        "                total = mask.sum().item()\n",
        "                accuracy = correct / total if total > 0 else 0\n",
        "\n",
        "            history['loss'].append(loss.item())\n",
        "            history['accuracy'].append(accuracy)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch:3d} | Loss: {loss.item():.4f} | Eve Accuracy: {accuracy*100:.1f}%\")\n",
        "\n",
        "            self.sched_eve.step(loss)\n",
        "\n",
        "        for param in self.alice.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in self.bob.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        print(f\"\\nStage 2 Complete!\")\n",
        "        self.current_stage = 3\n",
        "\n",
        "        return history\n",
        "\n",
        "    def train_stage3_adversarial(self, messages, num_epochs=100):\n",
        "        \"\"\"Stage 3: Adversarial fine-tuning with security focus\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"STAGE 3: ADVERSARIAL FINE-TUNING\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        history = {\n",
        "            'bob_loss': [], 'eve_loss': [],\n",
        "            'bob_acc': [], 'eve_acc': [], 'ratio': []\n",
        "        }\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            batch = np.random.choice(messages, size=min(6, len(messages)), replace=False).tolist()\n",
        "            tokens = self.processor.batch_encode(batch).to(self.device)\n",
        "            keys = self.key_manager.generate_keys_for_batch(len(batch), tokens.size(1))\n",
        "\n",
        "            # === Train Alice+Bob (reconstruction) ===\n",
        "            self.opt_alice.zero_grad()\n",
        "            self.opt_bob.zero_grad()\n",
        "\n",
        "            self.alice.train()\n",
        "            self.bob.train()\n",
        "\n",
        "            encrypted = self.alice(tokens, keys['key_tensors'])\n",
        "            bob_logits = self.bob(encrypted, keys['key_tensors'])\n",
        "            bob_loss = self.ce_loss(bob_logits.reshape(-1, bob_logits.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            bob_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.alice.parameters(), 0.5)\n",
        "            torch.nn.utils.clip_grad_norm_(self.bob.parameters(), 0.5)\n",
        "            self.opt_alice.step()\n",
        "            self.opt_bob.step()\n",
        "\n",
        "            # === Train Eve (attack) ===\n",
        "            self.opt_eve.zero_grad()\n",
        "            self.eve.train()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                encrypted_for_eve = self.alice(tokens, keys['key_tensors'])\n",
        "\n",
        "            eve_logits = self.eve(encrypted_for_eve)\n",
        "            eve_loss = self.ce_loss(eve_logits.reshape(-1, eve_logits.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            eve_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.eve.parameters(), 1.0)\n",
        "            self.opt_eve.step()\n",
        "\n",
        "            # === Adversarial training for Alice (fool Eve) ===\n",
        "            self.opt_alice.zero_grad()\n",
        "\n",
        "            encrypted_adv = self.alice(tokens, keys['key_tensors'])\n",
        "            eve_logits_adv = self.eve(encrypted_adv)\n",
        "\n",
        "            # Alice wants to MAXIMIZE Eve's loss (make Eve confused)\n",
        "            adv_loss = -self.ce_loss(eve_logits_adv.reshape(-1, eve_logits_adv.size(-1)), tokens.reshape(-1))\n",
        "\n",
        "            # Weight adversarial loss\n",
        "            (adv_loss * 0.3).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.alice.parameters(), 0.5)\n",
        "            self.opt_alice.step()\n",
        "\n",
        "            # Calculate metrics\n",
        "            with torch.no_grad():\n",
        "                bob_pred = torch.argmax(bob_logits, dim=-1)\n",
        "                eve_pred = torch.argmax(eve_logits, dim=-1)\n",
        "                mask = tokens != 0\n",
        "\n",
        "                bob_acc = ((bob_pred == tokens) & mask).sum().item() / mask.sum().item()\n",
        "                eve_acc = ((eve_pred == tokens) & mask).sum().item() / mask.sum().item()\n",
        "                ratio = eve_loss.item() / (bob_loss.item() + 1e-8)\n",
        "\n",
        "            history['bob_loss'].append(bob_loss.item())\n",
        "            history['eve_loss'].append(eve_loss.item())\n",
        "            history['bob_acc'].append(bob_acc)\n",
        "            history['eve_acc'].append(eve_acc)\n",
        "            history['ratio'].append(ratio)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch:3d} | Bob: {bob_loss.item():.3f} ({bob_acc*100:.1f}%) | \"\n",
        "                      f\"Eve: {eve_loss.item():.3f} ({eve_acc*100:.1f}%) | Ratio: {ratio:.2f}x\")\n",
        "\n",
        "            self.sched_alice.step()\n",
        "            self.sched_bob.step()\n",
        "            self.sched_eve.step(eve_loss)\n",
        "\n",
        "        print(f\"\\nStage 3 Complete!\")\n",
        "        return history\n",
        "\n",
        "    def full_training(self, messages, stage1_epochs=150, stage2_epochs=50, stage3_epochs=100):\n",
        "        \"\"\"Run all 3 stages\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"FULL STAGED TRAINING\")\n",
        "        print(f\"Dataset: {len(messages)} messages\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        h1 = self.train_stage1_reconstruction(messages, stage1_epochs)\n",
        "        h2 = self.train_stage2_adversary(messages, stage2_epochs)\n",
        "        h3 = self.train_stage3_adversarial(messages, stage3_epochs)\n",
        "\n",
        "        return {'stage1': h1, 'stage2': h2, 'stage3': h3}\n",
        "\n",
        "\n",
        "class CryptoEvaluator:\n",
        "    @staticmethod\n",
        "    def evaluate(trainer, test_messages):\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"FINAL EVALUATION\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        results = {'bob_sim': [], 'eve_sim': [], 'examples': []}\n",
        "\n",
        "        for i, original in enumerate(test_messages[:10]):\n",
        "            tokens = trainer.processor.encode(original).unsqueeze(0).to(trainer.device)\n",
        "            keys = trainer.key_manager.generate_keys_for_batch(1, tokens.size(1))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                encrypted = trainer.alice(tokens, keys['key_tensors'])\n",
        "                bob_logits = trainer.bob(encrypted, keys['key_tensors'])\n",
        "                eve_logits = trainer.eve(encrypted)\n",
        "\n",
        "                bob_tokens = torch.argmax(bob_logits, dim=-1)\n",
        "                eve_tokens = torch.argmax(eve_logits, dim=-1)\n",
        "\n",
        "                bob_msg = trainer.processor.decode(bob_tokens[0])\n",
        "                eve_msg = trainer.processor.decode(eve_tokens[0])\n",
        "\n",
        "            bob_sim = SequenceMatcher(None, original, bob_msg).ratio()\n",
        "            eve_sim = SequenceMatcher(None, original, eve_msg).ratio()\n",
        "\n",
        "            results['bob_sim'].append(bob_sim)\n",
        "            results['eve_sim'].append(eve_sim)\n",
        "            results['examples'].append((original, bob_msg, eve_msg))\n",
        "\n",
        "            if i < 5:\n",
        "                print(f\"\\nExample {i+1}:\")\n",
        "                print(f\"  Original: '{original}'\")\n",
        "                print(f\"  Bob:      '{bob_msg}' ({bob_sim*100:.1f}%)\")\n",
        "                print(f\"  Eve:      '{eve_msg}' ({eve_sim*100:.1f}%)\")\n",
        "\n",
        "        avg_bob = np.mean(results['bob_sim'])\n",
        "        avg_eve = np.mean(results['eve_sim'])\n",
        "\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(f\"Bob Similarity: {avg_bob*100:.1f}% {'✓' if avg_bob > 0.9 else '✗'}\")\n",
        "        print(f\"Eve Similarity: {avg_eve*100:.1f}% {'✓' if avg_eve < 0.3 else '✗'}\")\n",
        "        print(f\"Security Ratio: {avg_bob/max(avg_eve,0.01):.2f}x\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Extended dataset\n",
        "    messages = np.array([\n",
        "        \"Hello World!\",\n",
        "        \"This is a test.\",\n",
        "        \"Secret message here.\",\n",
        "        \"Encryption works!\",\n",
        "        \"Neural crypto system.\",\n",
        "        \"Testing ABC 123.\",\n",
        "        \"Quick brown fox.\",\n",
        "        \"The lazy dog jumps.\",\n",
        "        \"Secure communication.\",\n",
        "        \"Privacy matters most.\",\n",
        "        \"Data protection now.\",\n",
        "        \"Hidden information.\"\n",
        "    ])\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    trainer = ImprovedCryptoTrainer(\n",
        "        vocab_size=98,\n",
        "        embed_dim=128,\n",
        "        alice_layers=4,\n",
        "        bob_layers=4,\n",
        "        eve_layers=6,\n",
        "        max_len=64,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    # Train with better settings\n",
        "    history = trainer.full_training(\n",
        "        messages,\n",
        "        stage1_epochs=150,\n",
        "        stage2_epochs=50,\n",
        "        stage3_epochs=100\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    results = CryptoEvaluator.evaluate(trainer, messages)\n",
        "\n",
        "    print(\"\\n✓ Training complete!\")\n",
        "    print(f\"\\nKey Improvements:\")\n",
        "    print(\"  - 8 chaotic keys (1-DEC + tent maps)\")\n",
        "    print(\"  - 4 key-mixing layers in Alice\")\n",
        "    print(\"  - 6-layer transformer for Eve\")\n",
        "    print(\"  - Adversarial training to fool Eve\")\n",
        "    print(\"  - Better optimization (AdamW + cosine annealing)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI1W6NORFalB",
        "outputId": "f4dff218-8888-4722-ff7e-0a3cb83903f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "FULL STAGED TRAINING\n",
            "Dataset: 12 messages\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "STAGE 1: TRAINING ALICE+BOB FOR RECONSTRUCTION\n",
            "======================================================================\n",
            "Epoch   0 | Loss: 4.6324 | Accuracy: 0.0%\n",
            "Epoch  10 | Loss: 2.8628 | Accuracy: 38.8%\n",
            "Epoch  20 | Loss: 2.3237 | Accuracy: 56.2%\n",
            "Epoch  30 | Loss: 1.3141 | Accuracy: 79.8%\n",
            "Epoch  40 | Loss: 0.7859 | Accuracy: 82.8%\n",
            "Epoch  50 | Loss: 0.6063 | Accuracy: 88.1%\n",
            "Epoch  60 | Loss: 0.5052 | Accuracy: 89.7%\n",
            "Epoch  70 | Loss: 0.3074 | Accuracy: 95.1%\n",
            "Epoch  80 | Loss: 0.1452 | Accuracy: 99.4%\n",
            "Epoch  90 | Loss: 0.0583 | Accuracy: 100.0%\n",
            "Epoch 100 | Loss: 0.0481 | Accuracy: 100.0%\n",
            "Early stopping at epoch 104\n",
            "\n",
            "Stage 1 Complete! Final accuracy: 100.0%\n",
            "SUCCESS: Bob can decrypt accurately!\n",
            "\n",
            "======================================================================\n",
            "STAGE 2: TRAINING EVE (ADVERSARY)\n",
            "======================================================================\n",
            "Epoch   0 | Loss: 4.6056 | Eve Accuracy: 0.0%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py:1340: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  current = float(metrics)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  10 | Loss: 3.0916 | Eve Accuracy: 74.8%\n",
            "Epoch  20 | Loss: 2.2587 | Eve Accuracy: 81.8%\n",
            "Epoch  30 | Loss: 1.5987 | Eve Accuracy: 92.9%\n",
            "Epoch  40 | Loss: 1.2161 | Eve Accuracy: 92.6%\n",
            "\n",
            "Stage 2 Complete!\n",
            "\n",
            "======================================================================\n",
            "STAGE 3: ADVERSARIAL FINE-TUNING\n",
            "======================================================================\n",
            "Epoch   0 | Bob: 0.035 (100.0%) | Eve: 0.996 (89.8%) | Ratio: 28.07x\n",
            "Epoch  10 | Bob: 0.042 (100.0%) | Eve: 0.763 (96.7%) | Ratio: 18.35x\n",
            "Epoch  20 | Bob: 0.061 (100.0%) | Eve: 1.154 (83.8%) | Ratio: 19.06x\n",
            "Epoch  30 | Bob: 0.053 (100.0%) | Eve: 0.828 (92.2%) | Ratio: 15.57x\n",
            "Epoch  40 | Bob: 3.696 (9.0%) | Eve: 5.167 (4.1%) | Ratio: 1.40x\n",
            "Epoch  50 | Bob: 3.377 (18.8%) | Eve: 4.463 (4.3%) | Ratio: 1.32x\n",
            "Epoch  60 | Bob: 3.639 (17.7%) | Eve: 5.035 (0.0%) | Ratio: 1.38x\n",
            "Epoch  70 | Bob: 1.927 (43.0%) | Eve: 5.530 (0.0%) | Ratio: 2.87x\n",
            "Epoch  80 | Bob: 1.711 (50.4%) | Eve: 5.822 (0.0%) | Ratio: 3.40x\n",
            "Epoch  90 | Bob: 1.203 (64.8%) | Eve: 5.866 (0.0%) | Ratio: 4.88x\n",
            "\n",
            "Stage 3 Complete!\n",
            "\n",
            "======================================================================\n",
            "FINAL EVALUATION\n",
            "======================================================================\n",
            "\n",
            "Example 1:\n",
            "  Original: 'Hello World!'\n",
            "  Bob:      'aekto portd' (52.2%)\n",
            "  Eve:      'oooooteottoooot' (22.2%)\n",
            "\n",
            "Example 2:\n",
            "  Original: 'This is a test.'\n",
            "  Bob:      'This is a cest.' (93.3%)\n",
            "  Eve:      'ootooeooeoeooooto ' (6.1%)\n",
            "\n",
            "Example 3:\n",
            "  Original: 'Secret message here.'\n",
            "  Bob:      'iecret messaie .ere.' (85.0%)\n",
            "  Eve:      'ooootooetooooooetototo' (14.3%)\n",
            "\n",
            "Example 4:\n",
            "  Original: 'Encryption works!'\n",
            "  Bob:      'oncryution .orks' (78.8%)\n",
            "  Eve:      'otootooootoetttooooi' (10.8%)\n",
            "\n",
            "Example 5:\n",
            "  Original: 'Neural crypto system.'\n",
            "  Bob:      'seural crypto sistem.' (90.5%)\n",
            "  Eve:      'ooootooeotoooteooooottoi' (17.8%)\n",
            "\n",
            "======================================================================\n",
            "Bob Similarity: 71.3% ✗\n",
            "Eve Similarity: 13.4% ✓\n",
            "Security Ratio: 5.30x\n",
            "======================================================================\n",
            "\n",
            "✓ Training complete!\n",
            "\n",
            "Key Improvements:\n",
            "  - 8 chaotic keys (1-DEC + tent maps)\n",
            "  - 4 key-mixing layers in Alice\n",
            "  - 6-layer transformer for Eve\n",
            "  - Adversarial training to fool Eve\n",
            "  - Better optimization (AdamW + cosine annealing)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3eCCdEsBFsfs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}