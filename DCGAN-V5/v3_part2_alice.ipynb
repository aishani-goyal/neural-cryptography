{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AbkHwsLTEGx1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class StrongKeyMixing(nn.Module):\n",
        "    \"\"\"Stronger key-dependent transformation\"\"\"\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super(StrongKeyMixing, self).__init__()\n",
        "        self.key_proj1 = nn.Linear(1, dim)\n",
        "        self.key_proj2 = nn.Linear(1, dim)\n",
        "        self.mixing = nn.Linear(dim * 2, dim)\n",
        "        self.gate = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x, key):\n",
        "        \"\"\"x: (B, L, D), key: (B, L)\"\"\"\n",
        "        k_exp = key.unsqueeze(-1)\n",
        "\n",
        "        # Multiple key projections\n",
        "        k1 = torch.tanh(self.key_proj1(k_exp))\n",
        "        k2 = torch.sigmoid(self.key_proj2(k_exp))\n",
        "\n",
        "        # Gated mixing\n",
        "        gate = torch.sigmoid(self.gate(x))\n",
        "        x_keyed = x * k1 + (1 - gate) * k2\n",
        "\n",
        "        # Non-linear mixing\n",
        "        combined = torch.cat([x, x_keyed], dim=-1)\n",
        "        mixed = self.mixing(combined)\n",
        "\n",
        "        return torch.tanh(mixed)\n",
        "\n",
        "\n",
        "class MultiLayerKeyEncryption(nn.Module):\n",
        "    \"\"\"Multiple encryption layers with different keys\"\"\"\n",
        "\n",
        "    def __init__(self, dim, num_key_layers=4):\n",
        "        super(MultiLayerKeyEncryption, self).__init__()\n",
        "        self.key_layers = nn.ModuleList([\n",
        "            StrongKeyMixing(dim) for _ in range(num_key_layers)\n",
        "        ])\n",
        "        self.norms = nn.ModuleList([\n",
        "            nn.LayerNorm(dim) for _ in range(num_key_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, keys):\n",
        "        \"\"\"Apply multiple key-dependent transformations\"\"\"\n",
        "        for i, (key_layer, norm) in enumerate(zip(self.key_layers, self.norms)):\n",
        "            key_idx = min(i, len(keys) - 1)\n",
        "            x = key_layer(x, keys[key_idx])\n",
        "            x = norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ImprovedAliceEncoder(nn.Module):\n",
        "    \"\"\"Stronger encryption with multiple key-dependent layers\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=98, embed_dim=128, num_layers=4, max_len=128):\n",
        "        super(ImprovedAliceEncoder, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        # Initial projection\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 2),\n",
        "            nn.LayerNorm(embed_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(embed_dim * 2, embed_dim),\n",
        "            nn.LayerNorm(embed_dim)\n",
        "        )\n",
        "\n",
        "        # Strong key-dependent encryption\n",
        "        self.key_encryption = MultiLayerKeyEncryption(embed_dim, num_key_layers=4)\n",
        "\n",
        "        # Additional encoding layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=4,\n",
        "                dim_feedforward=embed_dim * 4,\n",
        "                dropout=0.1,\n",
        "                activation='gelu',\n",
        "                batch_first=True\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Final non-linear transformation\n",
        "        self.output_proj = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, tokens, keys):\n",
        "        \"\"\"\n",
        "        tokens: (B, L)\n",
        "        keys: list of (B, L) tensors\n",
        "        \"\"\"\n",
        "        # Embed and project\n",
        "        x = self.embedding(tokens)\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        # Apply key-dependent encryption (critical!)\n",
        "        x = self.key_encryption(x, keys)\n",
        "\n",
        "        # Transformer encoding\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Final projection\n",
        "        x = self.output_proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class AliceEncryptor:\n",
        "    \"\"\"Wrapper for Alice encoder\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=98, embed_dim=128, num_layers=4, max_len=128,\n",
        "                 device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.max_len = max_len\n",
        "        self.alice = ImprovedAliceEncoder(vocab_size, embed_dim, num_layers, max_len).to(device)\n",
        "\n",
        "        from v3_part1 import StringProcessor\n",
        "        self.processor = StringProcessor(max_length=max_len)\n",
        "\n",
        "    def encrypt(self, messages, keys):\n",
        "        \"\"\"Encrypt messages\"\"\"\n",
        "        if isinstance(messages, list) and isinstance(messages[0], str):\n",
        "            tokens = self.processor.batch_encode(messages).to(self.device)\n",
        "        else:\n",
        "            tokens = messages.to(self.device)\n",
        "\n",
        "        seq_len = tokens.size(1)\n",
        "        truncated_keys = [k[:, :seq_len] for k in keys['key_tensors']]\n",
        "\n",
        "        self.alice.eval()\n",
        "        with torch.no_grad():\n",
        "            encrypted = self.alice(tokens, truncated_keys)\n",
        "\n",
        "        return encrypted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*70)\n",
        "    print(\"IMPROVED ALICE ENCODER V3\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    from v3_part1 import StringProcessor, KeyManager\n",
        "\n",
        "    processor = StringProcessor(max_length=64)\n",
        "    key_manager = KeyManager(device=device)\n",
        "    alice_enc = AliceEncryptor(\n",
        "        vocab_size=processor.vocab_size,\n",
        "        embed_dim=128,\n",
        "        num_layers=4,\n",
        "        max_len=64,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        \"Hello World!\",\n",
        "        \"Testing encryption system.\",\n",
        "        \"Short test\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nTest messages:\")\n",
        "    for i, msg in enumerate(messages):\n",
        "        print(f\"  {i+1}. '{msg}'\")\n",
        "\n",
        "    keys = key_manager.generate_keys_for_batch(\n",
        "        batch_size=len(messages),\n",
        "        seq_length=64\n",
        "    )\n",
        "\n",
        "    print(\"\\nEncrypting...\")\n",
        "    encrypted = alice_enc.encrypt(messages, keys)\n",
        "\n",
        "    print(f\"\\nEncryption complete!\")\n",
        "    print(f\"  Output shape: {encrypted.shape}\")\n",
        "    print(f\"  Sample encrypted: {encrypted[0, 0, :10]}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLvvXzI3ERZG",
        "outputId": "714b5a3a-52d3-4c66-d128-426734bf2be5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "IMPROVED ALICE ENCODER V3\n",
            "======================================================================\n",
            "\n",
            "Test messages:\n",
            "  1. 'Hello World!'\n",
            "  2. 'Testing encryption system.'\n",
            "  3. 'Short test'\n",
            "\n",
            "Encrypting...\n",
            "\n",
            "Encryption complete!\n",
            "  Output shape: torch.Size([3, 64, 128])\n",
            "  Sample encrypted: tensor([-0.7602,  0.7383, -0.4264, -0.2587,  0.1478,  0.2741, -0.5531, -0.5341,\n",
            "         0.4954,  0.1915])\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CUQzu7ulEY1t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}