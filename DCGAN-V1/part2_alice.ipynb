{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# ============ Helper Classes (Kept for reference/future use) ============\n",
        "# Note: Current implementation uses fully differentiable neural architecture\n",
        "# These classes can be used for additional security layers if needed\n",
        "\n",
        "class VPDEncoder:\n",
        "    \"\"\"Virtual Planet Domain encoding (non-differentiable)\"\"\"\n",
        "    def __init__(self, radius=1.0):\n",
        "        self.radius = radius\n",
        "\n",
        "    def encode(self, data):\n",
        "        if isinstance(data, torch.Tensor):\n",
        "            data = data.cpu().numpy()\n",
        "        data_norm = (data - data.min()) / (data.max() - data.min() + 1e-8)\n",
        "        theta = data_norm * 2 * np.pi\n",
        "        phi = data_norm * np.pi\n",
        "        x = self.radius * np.sin(phi) * np.cos(theta)\n",
        "        y = self.radius * np.sin(phi) * np.sin(theta)\n",
        "        z = self.radius * np.cos(phi)\n",
        "        return np.stack([x, y, z], axis=-1)\n",
        "\n",
        "\n",
        "class PixelIntershuffler:\n",
        "    \"\"\"3D pixel intershuffling (non-differentiable)\"\"\"\n",
        "    def __init__(self, key_sequence):\n",
        "        self.key_sequence = key_sequence\n",
        "\n",
        "    def generate_shuffle_indices(self, length):\n",
        "        indices = np.arange(length)\n",
        "        np.random.seed(int(np.sum(self.key_sequence * 1e10) % (2**32)))\n",
        "        np.random.shuffle(indices)\n",
        "        return indices\n",
        "\n",
        "    def shuffle(self, data):\n",
        "        original_shape = data.shape\n",
        "        flat_data = data.flatten()\n",
        "        indices = self.generate_shuffle_indices(len(flat_data))\n",
        "        shuffled = flat_data[indices]\n",
        "        return shuffled.reshape(original_shape), indices\n",
        "\n",
        "\n",
        "class ZigzagXOR:\n",
        "    \"\"\"Zigzag XOR operation (non-differentiable)\"\"\"\n",
        "    def __init__(self, key_sequences):\n",
        "        self.key_sequences = key_sequences\n",
        "\n",
        "    def apply_xor(self, data, round_num=0):\n",
        "        key = self.key_sequences[round_num % len(self.key_sequences)]\n",
        "        if len(key) < len(data):\n",
        "            key = np.tile(key, (len(data) // len(key) + 1))[:len(data)]\n",
        "        else:\n",
        "            key = key[:len(data)]\n",
        "        data_int = (data * 255).astype(np.uint8)\n",
        "        key_int = (key * 255).astype(np.uint8)\n",
        "        xor_result = np.bitwise_xor(data_int, key_int)\n",
        "        return xor_result.astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "# ============ Alice Network (Encryption) ============\n",
        "class AliceNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size=128, num_rounds=3):\n",
        "        super(AliceNetwork, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_rounds = num_rounds\n",
        "\n",
        "        # Simplified architecture for differentiability\n",
        "        # Encoder pathway\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size * 2, hidden_size * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size * 2, hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Key mixing layers\n",
        "        self.key_mixer = nn.ModuleList([\n",
        "            nn.Linear(input_size, hidden_size) for _ in range(num_rounds)\n",
        "        ])\n",
        "\n",
        "        # Encryption layers\n",
        "        self.encrypt_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_size * 2, hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_size, hidden_size),\n",
        "                nn.Tanh()\n",
        "            ) for _ in range(num_rounds)\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.output = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, key_sequences):\n",
        "        \"\"\"\n",
        "        Forward pass through Alice network\n",
        "        x: input message (batch_size, input_size)\n",
        "        key_sequences: list of 8 chaotic key sequences\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        device = x.device\n",
        "\n",
        "        # Convert key sequences to tensor\n",
        "        keys_tensor = torch.FloatTensor(np.array(key_sequences[:self.num_rounds])).to(device)\n",
        "\n",
        "        # Initial encoding\n",
        "        encoded = self.encoder(x)\n",
        "\n",
        "        # Multiple rounds of key-based encryption\n",
        "        for round_idx in range(self.num_rounds):\n",
        "            # Mix with key\n",
        "            key_features = self.key_mixer[round_idx](keys_tensor[round_idx].unsqueeze(0).expand(batch_size, -1))\n",
        "\n",
        "            # Combine message and key features\n",
        "            combined = torch.cat([encoded, key_features], dim=1)\n",
        "\n",
        "            # Encrypt\n",
        "            encoded = self.encrypt_layers[round_idx](combined)\n",
        "\n",
        "        # Final output\n",
        "        ciphertext = self.output(encoded)\n",
        "\n",
        "        return ciphertext\n",
        "\n",
        "\n",
        "# ============ Complete Alice Encryption Module ============\n",
        "class AliceEncryption:\n",
        "    def __init__(self, input_size, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        self.alice = AliceNetwork(input_size).to(device)\n",
        "\n",
        "    def encrypt(self, message, key_sequences):\n",
        "        \"\"\"\n",
        "        Encrypt message using Alice network\n",
        "        message: torch.Tensor or numpy array\n",
        "        key_sequences: list of 8 chaotic sequences from DCGAN key generator\n",
        "        \"\"\"\n",
        "        if isinstance(message, np.ndarray):\n",
        "            message = torch.FloatTensor(message)\n",
        "\n",
        "        # Move message to the same device as the model\n",
        "        message = message.to(self.device)\n",
        "\n",
        "        if len(message.shape) == 1:\n",
        "            message = message.unsqueeze(0)\n",
        "\n",
        "        self.alice.eval()\n",
        "        with torch.no_grad():\n",
        "            ciphertext = self.alice(message, key_sequences)\n",
        "\n",
        "        return ciphertext\n",
        "\n",
        "    def train_step(self, message, key_sequences, optimizer):\n",
        "        \"\"\"Single training step for Alice\"\"\"\n",
        "        self.alice.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        ciphertext = self.alice(message, key_sequences)\n",
        "\n",
        "        return ciphertext\n",
        "\n",
        "\n",
        "# ============ Usage Example ============\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    input_size = 256\n",
        "    batch_size = 4\n",
        "\n",
        "    # Determine device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Initialize Alice\n",
        "    alice_enc = AliceEncryption(input_size, device=device)\n",
        "\n",
        "    # Create dummy message and keys (on the correct device)\n",
        "    message = torch.randn(batch_size, input_size).to(device)\n",
        "\n",
        "    # Simulate key sequences from Part 1\n",
        "    key_sequences = [np.random.rand(input_size) for _ in range(8)]\n",
        "\n",
        "    print(\"Encrypting message...\")\n",
        "    ciphertext = alice_enc.encrypt(message, key_sequences)\n",
        "\n",
        "    print(f\"Message shape: {message.shape}\")\n",
        "    print(f\"Ciphertext shape: {ciphertext.shape}\")\n",
        "    print(f\"Encryption completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boE1wpbGJnuM",
        "outputId": "23270d01-04dd-4ec2-8c6c-5db2af84a53b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Encrypting message...\n",
            "Message shape: torch.Size([4, 256])\n",
            "Ciphertext shape: torch.Size([4, 256])\n",
            "Encryption completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "35y9JSgkKMe-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}